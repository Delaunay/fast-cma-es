:encoding: utf-8
:imagesdir: img
:cpp: C++

== Optimization of functions with multiple objectives

The code for this tutorial is at 
https://github.com/dietmarwo/fast-cma-es/blob/master/examples/moexamples.py[moexamples.py] and uses 
https://github.com/dietmarwo/fast-cma-es/blob/master/fcmaes/moretry.py[moretry.py] . 
We show:

- How fcmaes single objective optimization algorithms together with a specialized parallel retry implementation
can be applied to compute the pareto front of multi objective problems.
- That good performance of easy benchmark problems (which can be solved by random sampling) is no performance 
indicator for real world problems. 
- How the weighted sum approach can be applied to compute the pareto front thereby parallelizing function evaluations.
- That standard algorithms like NGSAII, if not parallelized, can be several orders of magnitude slower than fcmaes multi objective parallel retry for real world problems. Scaling achieved by parallelization is only part of the story. 

=== Experiments with artificial benchmark functions

Lets first investigate some artificial multi-objective benchmark functions from
https://github.com/DEAP/deap/blob/master/deap/benchmarks/[deap-tests].
See https://www.sciencedirect.com/science/article/pii/S2352711020300911[MOBOpt]
for a description of these functions. As in this paper we chose the well known
benchmark functions Schaffer, Fonseca, Poloni and ZDT1. 

First lets check if we need any optimization algorithms at all by simply evaluating
500000 random samples. Even a single threaded execution takes only about 5 seconds
on our AMD 5950x 16 core machine:

[source,python]
----
def monte_carlo(fun, bounds, n=500000, value_limits=None):
    rg = Generator(MT19937())
    ys = []
    for _ in range(n):
        x = rg.uniform(bounds.lb, bounds.ub)
        y = fun(x)
        if value_limits is None or all([y[i] < value_limits[i] for i in range(len(y))]):
            ys.append(y)
    return np.array(ys)
----

- Fonseca function random samples:

image::monte_fonseca.png[] 

- Poloni function random samples:

image::monte_poloni.png[] 

Python is surprisingly fast converting these results into a pareto front showing all non-dominated solutions. We change the code above to add this conversion. 

[source,python]
----
    ...
    ys = np.array(ys)
    pareto = moretry._pareto(ys)
    return ys[pareto]
----

- Fonseca function, dim = 20, random samples pareto front, time = 8.19 sec:

image::monte_fonseca_front.png[] 

- Poloni function, dim = 20, random samples pareto front, time = 5.81 sec:

image::monte_poloni_front.png[]

Lets try two more benchmark problems:

- Schaffer function, dim = 20, random samples pareto front, time = 8.19 sec:

image::monte_schaffer_front.png[]

- ZDT1 function, dim = 20, random samples pareto front, time = 7.34 sec:

image::monte_zdt1_front.png[]

Wait, the ZDT1 result is not what we expected, may be it helps if we take 200 million random samples?

 - ZDT1 function 200 million random samples pareto front, time = 2846 sec:

image::monte_zdt1_200M_front.png[]

We needed 2846 seconds and a lot of memory for this computation and still didn't get a decent result. 

What did we learn so far? For most problems no sophisticated optimization algorithm is needed, we can 
compute the pareto front in seconds using random samples? Not really, but we learned another 
interesting fact: These "benchmark" functions are designed to show potential flaws in 
multi objective optimization algorithms - as we will see later.
They don't reflect typical real world problems. You should not predict the
performance of an algorithm for real world problems using these benchmarks. For this reason
lets switch our focus to a 

=== Real World Multi Objective Scenario

Suppose we work at NASA and our task is the planning of the 
https://solarsystem.nasa.gov/missions/cassini/overview/[Cassini] mission to Saturn. 
Fortunately our colleagues at ESA prepared a nice model 
https://www.esa.int/gsp/ACT/projects/gtop/cassini1/[Cassini model] we can adapt to create
a multi objective fitness function. Our boss told us that the overall 
mission time should be < 2000 days. He leaves in a few hours for a big planning meeting and
we need to convince him until then that this is a stupid idea. We need to show him the
tradeoff between fuel consumption and mission time, which means we have to compute the
pareto front for these two competing objectives. Not enough time to feed our Supercomputer,
we only have a fast 16 core desktop (AMD 5950x) available for the analysis. 

We import ESAs single objective Cassini fitness function which determines the overall delta
velocity, which is more or less equivalent to the fuel consumption. The second
objective, the travel time, can easily be derived from the input arguments. 

[source,python]
----
from fcmaes.astro import Cassini1

class cassini1_mo: 

    def __init__(self):
        self.base = Cassini1()
        self.bounds = self.base.bounds
 
    def fun(self, x):
        dv = self.base.fun(np.array(x)) # delta velocity, original objective (km/s)
        mission_time = sum(x[1:]) # mission time (days)
        y = np.empty(2)
        y[0] = dv       
        y[1] = mission_time
        return y
----

From the https://github.com/dietmarwo/fast-cma-es/blob/master/README.adoc[Readme] we know that the first
objective has an optimal value of 4.93 km/s. It is the easiest of the GTOP problems, solvable 
in under 10 seconds. Will the multi objective version be as easy to solve?
Considering the ZDT1 results above we are skeptical if random sampling will lead us anywhere. 
But since it is single threaded we can start multiple experiments simultaneously. First the 
result with 500000 random samples. 

- Cassini1 function 500000 random samples, time = 12.4 sec:

image::monte_cassini1_mo_front.png[]

Oops this is really bad. So lets try 200 million samples:

- Cassini1 function 200 million random samples, time = 5207.44 sec:

image::monte_cassini1_mo_200M_front.png[]

5207 seconds for this? Not enough to convince our boss. May be the Cassini problem is not as 
easy as the single objective results may suggest. 

=== NSGA-II Non-dominated Sorting Genetic Algorithm

But there is an alternative, lets try the well known https://pymoo.org/algorithms/nsga2.html[NSGA-II] algorithm. We adapted the code from https://github.com/ppgaluzio/MOBOpt/blob/master/mobopt/_NSGA-II.py[NSGA-II.py] for this experiment. 

[source,python]
----
def nsgaII_test(problem, fname, NGEN=2000, MU=100, value_limits = None):
    time0 = time.perf_counter() # optimization start time
    name = problem.__class__.__name__ 
    logger().info('optimize ' + name + ' nsgaII') 
    pbounds = np.array(list(zip(problem.bounds.lb, problem.bounds.ub)))
    pop, logbook, front = nsgaII(2, problem.fun, pbounds, NGEN=NGEN, MU=MU) 
    logger().info(name + ' nsgaII time ' + str(dtime(time0)))    
    if not value_limits is None:
        front = np.array(
            [y for y in front if all([y[i] < value_limits[i] for i in range(len(y))])])
    moretry.plot(front, 'nsgaII_' + name + fname)
----

Unfortunately the implementation is single threaded, but NSGA-II solves all our benchmark problems in under 30 seconds:

- Fonseca function, dim = 20, NSGA-II pareto front, NGEN=2000, MU=100, time = 22.38 sec:

image::nsgaII_fonseca_front.png[] 

- Poloni function, dim = 20, NSGA-II pareto front, NGEN=2000, MU=100, time = 20.52 sec:

image::nsgaII_poloni_front.png[]

- Schaffer function, dim = 20, NSGA-II pareto front, NGEN=2000, MU=100, time = 15.01 sec:

image::nsgaII_schaffer_front.png[]

- ZDT1 function, dim = 20, NSGA-II pareto front, NGEN=2000, MU=100, time = 26.39 sec:

image::nsgaII_zdt1_front.png[]

Encouraged by the good and fast results for the artificial benchmarks
we hope NSGA-II should also solve the Cassini problem. We expect it to be harder, therefore
we start with 20000 generations and a population size of 200.  

- Cassini1 function NSGA-II pareto front, NGEN=20000, MU=200, time = 1080.23 sec:

image::nsgaII_cassini1_mo_20k200_front.png[]

We already need 1080 sec, but the results are not convincing. So we incrementally
increase the number of generations further. 

- Cassini1 function NSGA-II pareto front, NGEN=40000, MU=200, time = 2140.0 sec:

image::nsgaII_cassini1_mo_40k200_front.png[]

- Cassini1 function NSGA-II pareto front, NGEN=80000, MU=200, time = 4409.71 sec:

image::nsgaII_cassini1_mo_80k200_front.png[]

- Cassini1 function NSGA-II pareto front, NGEN=120000, MU=200, time = 6587.19 sec:

image::nsgaII_cassini1_mo_120k200_front.png[]

A bit disappointing. Even with 120000 generations and a population size of 200, taking about 6587 seconds,
we still get a quite mediocre result. 

=== fcmaes multi objective parallel retry

Our boss is leaving soon, we are running out of time. Perhaps there is a way to apply our
fast - and parallelizable - single objective algorithms. What if we wrap the multi-objective
function and map it to a single objective one using the weighted sum approach?

[source,python]
----
class mo_wrapper(object):
   
    def __init__(self, fun, weights, y_exp=2):
        self.fun = fun  
        self.nobj = len(weights)
        self.weights = weights 
        self.y_exp = y_exp
 
    def eval(self, x):
        y = self.fun(np.array(x))
        return sum([self.weights[i] * (y[i]**self.y_exp) 
                    for i in range(self.nobj)])**(1.0/self.y_exp)
----

The idea is now to use random weights - inside defined boundaries - for each optimization retry.
Since these retries are executed in parallel, we can compute much more function evaluations per second
this way. Why do we need a configurable exponent `y_exp` ?
For problems where the pareto front contains very different values for the objectives,
like the Poloni function, we need a low exponent:

- Poloni weighted sum,  y_exp = 1.0, 2000 evals, 1024 retries, 2.7 sec:

image::poloni_1.0_cma_front.png[]

Using a higher exponent we would loose the extreme values at the left.
For real world problems usually we are not interested in results where one of our objectives
has a bad value, we prefer balanced results. For the cassini mission there even may be hard limits 
for both travel time and fuel consumption.  

On the other hand for functions like Fonseca we would have a pareto front "gap"
in the middle for low exponents, therefore we increase it to 3.0:

- Fonseca weighted sum, y_exp = 3.0, 2000 evals, 1024 retries, 4.9 sec:

image::fonseca_3.0_decma_front.png[]

For real world problems `y_exp = 2.0`, the default value usually is a good choice. 

We configure 1024 retries with a maximum of 50000 evaluations. Since our processor supports 32 parallel threads we choose a number of retries dividable by 32. 

[source,python]
----
def mo_retry(problem, opt, fname, value_limits = None, num_retries = 1024, exp = 2.0):
    time0 = time.perf_counter() # optimization start time
    name = problem.name 
    logger().info('optimize ' + name + ' ' + opt.name) 
    xs, ys = moretry.minimize(problem.fun,
             problem.bounds, problem.weight_bounds, 
             value_exp = exp,
             value_limits = value_limits,
             num_retries = num_retries,              
             optimizer = opt,
             logger=logger())
    np.savez_compressed(name + '_' + fname, xs=xs, ys=ys)
    xs, front = moretry.pareto(xs, ys)
    logger().info(name + ' ' + opt.name + ' time ' + str(dtime(time0))) 
    moretry.plot(front, name + '_' + fname + '.png')
[source,python]
----

- Cassini weighted sum, 1024 retries, max 50000 evals, BiteOpt algorithm, time = 43.62 sec:

image::cassini_bite_front.png[]
 
- Cassini weighted sum, 1024 retries, max 50000 evals, DE-CMA sequence, time = 31.94 sec:

image::cassini_decma_front.png[]

Taking only a few seconds fcmaes parallel retry outperforms NGSAII by factor 200 delivering a superior result. The only drawback is that we have to extend the function definition by `weight_bounds` which
sets the bounds for the randomly generated objective weights. The first objective is in m/s, optimum
about 4.7 m/s, the second one in days, optimum > 1000 days. So we define   
`weight_bounds = Bounds([10, 0.01], [1000, 1])` to balance the weighted sum: 

[source,python]
----
class cassini1_mo: 

    def __init__(self):
        self.base = Cassini1()
        self.bounds = self.base.bounds
        self.weight_bounds = Bounds([10, 0.01], [1000, 1]) # weighting of objectives
----

Finally we got our Cassini pareto front to convince our boss to allow for a maximal mission time of 2100 days. 

Note that we write of the optimization results before applying ``moretry.pareto` which
can be plotted later if needed:

[source,python]
----
    with np.load('fname.npz') as data:
        xs = data['xs']
        ys = data['ys']
        moretry.plot(ys, 'fname.png', interp=False)
----

- Cassini weighted sum, 1024 retries, max 50000 evals, DE-CMA sequence, all optimization results:

image::cassAll.png[]

=== Constraints

What if our problem has to fulfill a list of constraints? They can be converted into objectives:

- Equality:  `a = b` can be converted into objective `abs(a-b)`
- Inequality: `a < b` can be converted into objective `max(0, a-b)`

Use high values as weight bounds, like `[1000, 1000]` allowing for no variation of constraint weights. 
Sometimes it is useful to add a constant penalty `c`:

- Equality:  `a = b` can be converted into objective `abs(a-b) + c if abs(a-b) > 0 else 0`
- Inequality: `a < b` can be converted into objective `a-b + c if a-b > 0 else 0`

=== What if the problem is crazy hard ?

Now we will show what you can do if your problem tests the limits of state of the art single
objective optimizers. Lets have a look at the unconstrained variant
of ESAs https://www.esa.int/gsp/ACT/projects/gtop/tandem/[Tandem] problem, 
another interplanetary trajectory with multiple planet gravity assist maneuvers.

Note that it took about 3 years until a 1673.88 kg solution
was discovered by G. Stracquadanio, A. La Ferla and G. Nicosia at University of Catania, see
https://www.esa.int/gsp/ACT/projects/gtop/tandem_unc . As usual we import the GTOP probem
and modify it to take the mission time as second objective into account:

[source,python]
----
from fcmaes.astro import Tandem

class tandem_mo: 

    def __init__(self, constrained=False):
        self.base = Tandem(5, constrained=constrained)
        self.bounds = self.base.bounds
        self.weight_bounds = Bounds([1, 0], [1, 0]) # ignore 2nd objective
        self.name = self.base.name
 
    def fun(self, x):
        final_mass = self.base.fun(np.array(x)) # original objective (-kg)
        mission_time = sum(x[4:8]) # mission time (days)
        y = np.empty(2)
        y[0] = final_mass       
        y[1] = mission_time
        return y

monte(tandem_mo(),'200M_front1.png', value_limits = [0, 10000], n=200000000)
----

Lets start with 200 million random samples:

- Tandem unconstrained, 200 million random samples, time = 23537 sec

image::monte_Tandem_mo 6_200M_front.png[]

Over 6 hours a for a maximal mass of 25 kg. A clear indication that this problem is really hard. 

==== NSGA-II

Since we got no chance using the random sample approach - no surprize - lets try NSGA-II next:

[source,python]
----
nsgaII_test(tandem_mo(), '_front.png', NGEN=120000, MU=200, value_limits = [0, 10000])
----

- Tandem unconstrained, NSGA-II pareto front, NGEN=120000, MU=200, time = 7245 sec

image::nsgaII_Tandem_mo_120k200front.png[]

Took over 2 hours, looks very smooth, but very far away from the real pareto front. Below 3000 days travel
time there are hardly much better solutions, but no one forced NSGA-II to avoid longer trajectories. 
The second objective seems to "drag" the algorithm away from good solutions. With the weighted sum approach
we have the means to fight this issue.  

==== fcmaes parallel retry

[source,python]
----
 mo_retry(tandem_mo(), de_cma(100000), '_de_cma_front' + str(i), num_retries=4096, exp=1.0)
----

- Tandem unconstrained, parallel retry de_cma, 100000 evaluations, 4096 retries, time = 556 sec

image::de_cma_tandem mo_front.png[]

To handle the complexity of the problem we increased the number of evaluations per retry to 100000. 
To fight the "drag" to low mission time solutions 
we completely block the second objective `weight_bounds = Bounds([1, 0], [1, 0])`
and use `exp=1.0` which makes the weighted sum identical to the first objective. This means
that alternatively we directly could have used the single objective Tandem version. We did
not to enable the following

==== Excercise

Experiment with other `weight_bounds` and `exp` settings. You will observe that preserving the
first objective unaltered is crucial to success. Experiment also with other algorithms, 
Bite_cpp(100000, M=16) probably being the strongest - for many other problems even superior - competitor. 

Since we use only the first objective for optimization, why not try the advanced retry which 
uses a smart management of the boundaries depending on previous runs. We feed the algorithm with
`problem.base.fun`, the single objective version of the Tandem problem. 
The pareto front is computed using `ys = np.array([problem.fun(x) for x in xs])`, the 
multi objective Tandem function applied to the optimization result.

[source,python]
----
from fcmaes import advretry

def mo_adv_retry(problem, opt, fname, value_limit = math.inf, num_retries = 1024):
    time0 = time.perf_counter() # optimization start time
    name = problem.name 
    logger().info('smart optimize ' + name + ' ' + opt.name) 
    store = advretry.Store(problem.bounds, capacity=500, logger=logger(), 
                           num_retries=num_retries) 
    ret = advretry.retry(problem.base.fun, store, opt.minimize, num_retries, value_limit)
    xs = np.array(store.get_xs())
    ys = np.array([problem.fun(x) for x in xs])
    np.savez_compressed(name + '_' + fname, xs=xs, ys=ys)
    xs, front = moretry.pareto(xs, ys)
    logger().info(name + ' ' + opt.name + ' time ' + str(dtime(time0))) 
    moretry.plot(front, name + '_' + fname + '.png')

mo_adv_retry(tandem_mo(), de_cma(1500), '_smart_front'+ str(i), value_limit = 0, num_retries = 20*4096)
----

- Tandem unconstrained, parallel smart retry de_cma, 20*4096 retries between 1500 and 75000 evaluations, time = 2358 sec

image::smart_tandem mo_front.png[]

==== Joined forces

A single run may be not sufficient for the pareto front, this is the reason we saved the optimization results
using `np.savez`. Now we can just collect these results to produce the final result using: 

[source,python]
----
def plot_all(folder, fname):
    files = glob.glob(folder + '/*.npz', recursive=True)
    xs = []
    ys = []
    for file in files:
        with np.load(file) as data:
            xs += list(data['xs'])
            ys += list(data['ys'])
    xs = np.array(xs); ys = np.array(ys)         
    xs, front = moretry.pareto(xs, ys)
    moretry.plot(ys, fname + '_all.png', interp=False)
    moretry.plot(front, fname + '_front.png')
----

We may distribute the computation over various machines / cluster nodes and combine the results later. 

- Tandem unconstrained, all combined optimization results:

image::tandem_mo_all.png[]

- Tandem unconstrained, pareto front of all combined optimization results:

image::tandem_mo_front.png[]

Note that we found the 1500 kg optimum for the constrained problem variant (travel time < 3650 days)
but still missed the 1673.88 kg unconstrained solution. Tell me if you know an algorithm which finds it reliably. 

=== What if the problem is not solvable even as single objective problem ?

In this case we need a surrogate model. See
https://www.sciencedirect.com/science/article/pii/S2352711020300911[MOBOpt], 
but for our example we will use an alternative approach. 

https://github.com/mlooz/pykep/blob/2edc5db4da9bdd5bec7326353a59c5a796d59ab3/pykep/trajopt/gym/_solar_orbiter.py#L753[_solar_orbiter_udp_1dsm] models 
the https://www.esa.int/Science_Exploration/Space_Science/Solar_Orbiter[Solar Orbiter] mission as a sequence of gravity assist maneuvers with a single deep space maneuver (1DSM) between the planets. Lets assume we use the planet sequence

[source,python]
----
seq=[earth, venus, venus, earth, venus, venus, venus, venus, venus, venus]
----

as in the original mission. The 1DSM solo model is very generic, it allows solutions not considered by the solo
planning team at ESOC. Unfortunately you need future optimization algorithms combined with an incredible amount of
computing power to solve it. So our first goal is to establish the correctness of the model by reproducing 
a number of good solutions we know already from a much simpler model which we fortunately already have here
https://github.com/dietmarwo/fast-cma-es/blob/master/examples/moexamples.py[solo_mgrar_udp.py]. Using 
this "surrogate" model we can compute solutions which are convertible into solutions of the 1DSM model. 
The conversion includes a 
local optimization using the 1dsm model for each surrogate solution because of accuracy issues. 
https://gist.github.com/dietmarwo/86f24e1b9a702e18615b767e226e883f[Here] we listed solutions for both solo models. 
There is no chance to apply existing multi-objective algorithms like NSGA-II neither to the 1DSM nor to the surrogate model. 

Solar Orbiter has not only two, but a number of competing primary objectives:

- Minimal delta velocity / fuel consumption
- Minimal overall travel time
- Maximal inclination relative to the sun equator - we want to investigate the poles of the sun. 
- Minimal - but limited - perhelion. We want to come close but avoid burning our equipment. 

Lets choose the following two objectives:

- First objective: maximal inclination in deg.
- Second objective: minimal travel time in days.

Solar Orbiter 1DSM model, all combined optimization results:

image::solo_mo_all.png[]

- Solar Orbiter 1DSM model, pareto front of all combined optimization results:

image::solo_mo_front.png[]

The pareto front is not very useful here, instead we use all good solutions and select one 
considering secondary objectives like:

- Do we cross a comet halo? The real solo mission does although this was not part of the planning
- Start velocity from earth
- Downlink capability - how fast can data be transferred during the mission

See https://issues.cosmos.esa.int/solarorbiterwiki/download/attachments/44993822/SOL-ESC-RP-05500%20-%20Issue%205r0%2C%20201681029%20-%20Solar%20Orbiter%20CReMA%20Issue%205%20Rev%200.pdf[SOL-ESC-RP-05500] for a detailed description
of the mission goals. 