:encoding: utf-8
:imagesdir: img

== How to win the optimization battle
To cite https://www.anandtech.com/show/15483/amd-threadripper-3990x-review/6[Anandtechs 3990x article] :
"With the right workload you win all battles". 
To win the optimization battle we have to create the
"right workload" for modern high core count processors. This means:

- Use stochastic methods. Many parallel retries - up to 128 with the 3990x desktop processor - are essentially for free if time is the constraint.
- Variation / diversity is more important than reaching consistently good results, we are targeting 
the global minimum. 
- We should aim for optimal scaling for high core count and not parallelize single optimization runs.
- Aim for high single thread performance of a single optimization retry specially for higher dimensions 
combined with a low memory footprint.
- The number of objective function evaluations needed should be minimized
- It should be possible to call very expensive objective function evaluations in parallel, even if not using parallel retry. 
- Multiple retries should be coordinated. Successful optimization runs should 
"exchange information" to improve the result. 
- Make use of SIMD instructions supported by modern processors to speed up optimization.   

We now compare four stochastic optimization methods available for Python users using the criteria above:

- fcmaes CMA-ES Python variant
- fcmaes CMA-ES C++ variant
- https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.dual_annealing.html[scipy dual annealing]
- https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html#scipy.optimize.differential_evolution[scipy differential evolution] 

applied to four real world optimization problems of different complexity:

- https://www.esa.int/gsp/ACT/projects/gtop/cassini1/[Cassini]
- https://www.esa.int/gsp/ACT/projects/gtop/gtoc1/[GTOC1]
- https://www.esa.int/gsp/ACT/projects/gtop/messenger_reduced/[Messenger reduced]
- https://www.esa.int/gsp/ACT/projects/gtop/messenger_full/[Messenger full]

which are accessible from Python via 
https://github.com/dietmarwo/fast-cma-es/blob/master/fcmaes/astro.py[astro.py] 
if you are on Linux or Windows.  

If you want to reproduce the results shown here, check 
https://github.com/dietmarwo/fast-cma-es/blob/master/fcmaes/examples.py[examples.py] and 
https://github.com/dietmarwo/fast-cma-es/blob/master/fcmaes/advexamples.py[advexamples.py]

===== Exercise
Verify that all other scipy algorithms perform worse with these four problems. 

For all these algorithms we test:

- 20 runs each performing 2000 retries each limiting the number of function evaluations to 50000.
- 20 runs each performing 4000 advanced coordinated retries with increasing number of function evaluations starting
with 2000, increasing each 100 retries by 1000.

These two methods are sufficient to solve the first three problems, for the last one we need:

- 2 runs each performing 50000 advanced coordinated retries with increasing number of function evaluations starting
with 2000, increasing each 100 retries by 1000 but with an absolute limit of 50000

The last test is also useful to determine the optimal number of coordinated retries per run, which differs depending 
on the problem and on the used optimization algorithm. 

These experiments can be reproduced by executing

	import fcmaes.examples
	import fcmaes.advexamples
	examples.test_all()
	advexamples.test_all()

Check optimizer.log for the results. 

==== Cassini1 problem

image::Ct4.png[]

===== Observations

- CMA-ES has a lower optimization algorithm overhead (higher evals/sec rate) as the other algorithms with
differential evolution being worst. 
- Dual annealing has highest, differential evolution the lowest number of evaluations per optimization. Note
that the limit was set to 50000 evaluations, but since Cassini1 is a quite easy task all algorithms need less
evaluations.
- Differential evolution, although competitive, is not able to reach the absolute minimum at 4.93. CMA-ES fights
with a local minimum at 5.3, but reaches the absolute minimum in under a minute in many runs.

image::Co4.png[]

===== Observations

- Coordinating the retries helps dual annealing and differential evolution.
- For CMA-ES the problem is too easy to gain from advanced retry. But it doesn't harm either, still the absolute 
minimum is reached in under a minute.

image::Co50.png[]

===== Observations

- 50000 optimizations per advanced retry run is overkill for the Cassini1 problem. 
- Good values are reached faster if you perform more shorter runs.
- This holds for all 4 optimization algorithms. 

==== GTOC1 problem

image::Gt4.png[]

===== Observations

- Although the problem has only 8 dimensions, it already quite tough. 
- CMA-ES uses less evaluations per optimization, upper limit is 50000 for all algorithms.
- Optimization overhead (evaluations/sec) is worst for differential evolution
- CMA-ES is faster overall and achieves superior results. 

image::Go4.png[]

===== Observations

- All four algorithms improve significantly from advanced coordinated retry. 
- Good values are reached much faster. 
- CMA-ES now often reaches the global minimum in less than 100 seconds. 

image::Go50.png[]

===== Observations

- Differential evolution and dual annealing improve with longer runs (50000 optimizations)
but still don't find the global minimum, even if we invest a full hour. 
- For CMA-ES executing multiple shorter runs is preferable

==== Messenger reduced problem

image::Mt4.png[]

===== Observations

- This problem, although for CMA-ES not harder than GTOC1, is too much for dual annealing and
differential evolution. Probably because these algorithms don't scale well with higher dimensions.
- Optimization algorithm overhead (evals/sec) is much higher than for GTOC1 for dual annealing and
differential evolution, but not for CMA-ES which debunks the myth that CMA-ES 
doesn't scale well with the number of dimensions.  
- CMA-ES is faster and achieves much better results using the same number of evaluations 
(max 50000 per retry).


image::Mo4.png[]

===== Observations

- All four algorithms improve significantly from advanced coordinated retry. 
- Differential evolution now finds good local minima. 
- CMA-ES often solves the problem in less than 100 seconds, but sometimes "hangs" at local minima
at 8.7 and 8.65. But already with the simple retry CMA-ES could often solve the problem fast.  

image::Mo50.png[]

===== Observations

- Only dual annealing profits from longer retry runs (50000 retries).
- Both dual annealing and differential evolution still miss the global minimum at 8.63. 
- For CMA-ES executing multiple shorter runs is preferable


==== Messenger full problem

image::Ft4.png[]

===== Observations

- This is the hardest problem tested here, it shows already with the simple retry the clear
superiority of CMA-ES which is much faster and achieves much better results if using the same
number of function evaluations (max 50000). 
- The evaluations/sec rate for dual annealing and differential evolution shrinks significantly
because the problem has 26 dimensions. Both algorithms scale bad for higher dimensions. 
- Surprisingly dual annealing is both faster and better than differential evolution for this problem
(if restricted to 50000 function evaluations) 
- In the literature you can find worse results for CMA-ES for this problem. This is probably because
here a lower relative initial stepsize - a random value between 0.05 and 0.1 - is used. This
increases the diversity / variance of the results. 

image::Fo4.png[]

===== Observations

- This time dual annealing and differential evolution gain more than CMA-ES using the advanced retry. 
- Still CMA-ES is by far the fastest algorithm delivering the best results here.
- The advantage for the C++ CMA-ES variant relative to the Python variant shrinks for higher 
dimensions since the BLAS / MKL calls start to dominate the overall performance.  

image::Fo50.png[]

===== Observations

- Longer retry runs (50000 coordinated advanced retries) only helps CMA-ES
- CMA-ES is able to find the global minimum in less than 1 hour - which is 
a bit lucky, sometimes the algorithm gets stuck at local minima at 2.4.
But keep in mind that the whole science community needed 8 years between 2009 and 2017
to find a 1.958 km/s solution. A 64 core processor solves the problem about 3 times faster.  

==== Summary

CMA-ES in combination with the advanced coordinated retry creates the
"right workload" for modern high core count processors, fulfilling all criteria
listed above. This way we can win the "optimization battle"

=== One more thing: Beyond Messenger full
Although https://www.esa.int/gsp/ACT/projects/gtop/messenger_full/[Messenger full] is 
a real challenge for optimization, its getting more interesting when we start
using optimization in areas where a search algorithm was required before. 
What do we gain from that?

* Since exhaustive search is almost never possible because of the exponential growth of the
search tree we have to implement heuristics selecting promising branches.
This way we may end up in a local optimum and miss the global one.
* The search algorithm has to be implemented / adapted to the specific problem, optimization is
easier to apply. 

As example we continue the analysis of the 
https://sophia.estec.esa.int/gtoc_portal/?page_id=13[GTOC1] competition from our
https://github.com/dietmarwo/fast-cma-es/blob/master/Tutorial.adoc[Tutorial]. There we 
concluded that using the https://www.esa.int/gsp/ACT/projects/gtop/gtoc1/[ESA abstraction] 
we cannot reach the winning score of -1850000 from JPL. Adding more planets to the trajectory
helps, but there is another issue:

The number of revolutions around the sun is chosen according to the minimal deltaV (delta velocity)
at departure from a planet. Higher deltaV means we need more fuel. The choice of the
number of revolutions determines the incoming arc at the next planet 
and can turn out to be bad if we look at the whole trajectory. To 
find the optimum we have to perform a search branching over the number of revolutions
for each planet to planet transfer. The following picture illustrates parts of the
search tree:

image::revolutions.png[]

We chose the following planet sequence: 
EVVEVVEESJA (E = Earth, V = Venus, J = Jupiter, S = Saturn, A = incoming asteroid)
which results in an optimal score of around -1670000 using 
the old deterministic objective function. 

==== Replacing search by optimization

In most cases the locally optimal number of revolutions is globally optimal. We assign 
probabilities to the child nodes dependent on the local deltaV. High probabilities are
assigned to low deltaV (fuel) branches. Then we adapt the objective function to chose
a number of revolutions according to the assigned probability. This way the objective 
function becomes noisy / non-deterministic but we avoid the need for a search algorithm. 

Lets check the results. This time another processor is chosen, the 32 core AMD 2990WX, which
is known to have scaling issues because of its internal design. The coordinated parallel
retry mechanism scales well even on this processor as the results show:

image::gjo_cma170.png[]

The best solution scores around -1920000. Objective function evaluation takes a bit more time
since we have ten planet to planet transfers now. We get 970000 evaluations / sec compared 
to around 600000 on the AMD 3950x we used before. To compute a real GTOC1 solution this 
impulse based solution has to be converted into a low thrust trajectory. Here is 
a https://youtu.be/zk75TaJKG_8[video] of a GTOC1 solution using the EVVEVVEESJA sequence
I computed in 2018 using this method.
 





