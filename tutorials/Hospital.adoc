:encoding: utf-8
:imagesdir: img
:cpp: C++

= Reactive Programming and Real World Optimization

=== Reactive Programming

Recently https://blog.oakbits.com/introduction-to-rxpy.html[reactive programming]
has gained a lot of traction, reactive frameworks like https://spring.io/reactive[Spring Reactive]
get more and more adapted. 

=== Real World Optimization

Real world optimization recently became the focus of the research on optimization algorithms, see for instance the
https://www.th-koeln.de/informatik-und-ingenieurwissenschaften/gecco-2021-industrial-challenge-call-for-participation_82086.php[Hospital Management Problem], 
the optimization of hospital management during a pandemy.
Real world optimization usually involves expensive simulation based objective functions. 

=== How are these connected? 

A reactive program sees its environment as sequences of asynchronous events it receives and emits. 
In the context of optimization this means: An optimizer emits a stream of argument vectors and receives a stream of
results - computed by an expensive simulation based objective function. 
On the other side there is a reactive function evaluator, receiving argument vectors and emitting results. 

=== What is the advantage of reactive programming for optimization?

The function evaluator may decide to:
 
- Spawn separate threads/processes executing the simulation. See for instance 
  https://github.com/dietmarwo/fast-cma-es/blob/master/fcmaes/evaluator.py[evaluator.py]

- Spawn separate jobs in a cloud cluster. See for instance 
  https://github.com/dietmarwo/fcmaes-java/blob/master/temporal/src/main/java/fcmaes/temporal/core/OptimizerActivityImpl.java[OptimizerActivityImpl.java]
  which uses the https://temporal.io/[temporal.io] workflow management framework which optionally can distribute tasks (called activities)
  in a cloud environment. 
  
The Hospital Management Problem spawns https://docs.docker.com/engine/reference/run/[Docker runs] to execute its hospital simulation.  
In a kubernetes based cloud environment this could involve running a https://kubernetes.io/docs/tasks/job/[kubernetes job] which is
physically executed on a different CPU or even a different cluster. 

Spawning a separate process / running a kubernetes job is inherently asynchronous. 
It is done to speed up the optimization by distributing the expensive evaluation of the objective function. 
Problem is that the reactive function evaluator is only part of the solution. 
It requires a reactive counterpart: The optimizer. 

=== Bridging reactive flows and blocking calls

In an ideal world both sides, the optimizer and the parallel function evaluator are implemented using the reactive style
avoiding blocking calls. But often we have to deal with blocking APIs like 
https://github.com/AlgTUDelft/ExpensiveOptimBenchmark/blob/642056f8d94c7f953e50c3cd05bbbf9f39ad5c3d/expensiveoptimbenchmark/problems/base.py#L9[this].

A parallel function evaluator can serve as a bridge between these worlds in both directions:
- To the blocking function evaluation: https://github.com/dietmarwo/fast-cma-es/blob/85742f420dabd4130d6c052ef3201a532a535901/fcmaes/evaluator.py#L72['y = fun(x)'].
- To its caller it can provide a blocking API: https://github.com/dietmarwo/fast-cma-es/blob/85742f420dabd4130d6c052ef3201a532a535901/fcmaes/evaluator.py#L19[eval_parallel].

Unfortunately this "bridge" idea doesn't work at the optimizer side: An optimizer which expects its argument vectors sequentially cannot be parallelized. 
This is true for all optimizers in https://github.com/AlgTUDelft/ExpensiveOptimBenchmark/tree/master/expensiveoptimbenchmark/solvers[solvers]. 

=== Backpressure

https://medium.com/@jayphelps/backpressure-explained-the-flow-of-data-through-software-2350b3e77ce7[Backpressure] is the 
"Resistance or force opposing the desired flow of data through software."
For optimization this means: There is a limit where parallelization of objective function evaluation makes sense. 
For population based algorithms usually the number of parallel workers should not exceed the population size. 
This limit induces "backpressure" into the optimization data flow, the exchange of argument vectors and results. 
One of the advantages of reactive frameworks is that they support the configuration of backpressure limits propagated
over a chain of reactive processing flows.  
In the case of real world optimization backpressure defines the rate the parallel evaluator receives its argument vectors. 
To evaluate an optimization method we need results for different backpressure limits which corresponds to the 
maximum level of parallelization. Higher backpressure limit / parallelization usually means a larger population size 
which can slow down convergence but lowers the chance to get stuck at a local minimum. 

=== What does that mean for the optimizer?

A reactive optimizer needs an asynchronous event based interface. 
Lets have a look how the optimizers in https://github.com/AlgTUDelft/ExpensiveOptimBenchmark/tree/master/expensiveoptimbenchmark/solvers[solvers]
work, for instance https://github.com/AlgTUDelft/ExpensiveOptimBenchmark/blob/master/expensiveoptimbenchmark/solvers/CMA/wCMA.py[CMA]

[source,python]
---- 
def f(x):
...
        return r
...
res = fmin(f, x0 / cmascale, sigma0, options=opts)
----

A synchronous function f is forwarded to the optimizer `fmin` which means it is impossible to parallelize the evaluation of `f`. 
Reason is that the optimizer expects the results "in order". Only after returning a result we obtain a new argument vector
for evaluation. 

In the optimization context the asynchronous interface we need is known as "ask/tell" interface, for instance implemented in the 
https://facebookresearch.github.io/nevergrad/optimization.html[nevergrad] framework. fcmaes supports this interface
for the python variants of its differential evolution and CMA-ES implementations, and if multiple objectives and constraints
are involved, with its MODE optimizer. 

Providing a reactive/asynchronous interface imposes some restrictions for the optimization:

- Mainly population based algorithms can profit from parallel function evaluation.
- Population size should be >= the number of parallel threads/processes/jobs. 

=== Evaluating surrogate optimization algorithms

https://github.com/AlgTUDelft/ExpensiveOptimBenchmark[ExpensiveOptimBenchmark] aims at the evaluation of so called surrogate model
using real world problems. Problem is that in the "real world" when faced with such an optimization problem
you would exploit every possibility to parallelize function evaluation using multiple cores of your processor or even multiple processor
nodes in your cluster. The evaluation of surrogate models should be compared to these parallel algorithms, not to sequential ones. 

=== Optimizing the hospital management problem

Lets suppose we are a hospital manager interested in a solution of the 
https://www.th-koeln.de/informatik-und-ingenieurwissenschaften/gecco-2021-industrial-challenge-call-for-participation_82086.php[Hospital Management Problem]
implemented here https://github.com/AlgTUDelft/ExpensiveOptimBenchmark/blob/master/expensiveoptimbenchmark/problems/DockerHospitalBenchmark.py[DockerHospitalBenchmark.py]
See also https://gecco-2021.sigevo.org/Competitions#id_Real-World%20Multi-Objective%20Optimization%20Competition[Competition].

To justify the last hardware investment the manager first tries to execute standard optimization methods 
which are able to fully exploit all the fancy cores the newly bought machine provides. 

It is a single objective problem without separate constraints, so he 
could use https://facebookresearch.github.io/nevergrad/optimization.html[nevergrad]
or fcmaes, both support the asynchronous ask/tell interface and parallel function evaluation. Another alternative is ESAs Pygmo using 
https://esa.github.io/pygmo2/tutorials/using_archipelago.html[archipelagos], but this approach needs a lot of function calls and
parallel function evaluation doesn't work "out of the box". 

Since this is a fcmaes tutorial our virtual hospital manager chooses fcmaes, but nevergrad would work as well. 
We need to adapt 
https://github.com/AlgTUDelft/ExpensiveOptimBenchmark/blob/master/expensiveoptimbenchmark/problems/DockerHospitalBenchmark.py[DockerHospitalBenchmark.py] 
for use with fcmaes: 

- To monitor progress we need some variables shared across python processes and wrap `evaluate(xs)`.
- We add a method `optimize(self, popsize, max_evaluations)` calling the optimizer.
- We intitialize `self.ints`, a list of booleans indicating which variables are discrete. Used by the `de` algorithm. 

Most of the changes are for the super class `BaseProblem`. We adapt the other classes similarly. 
If on Linux you may replace the `de` and `cmaes` by their {cpp} counterparts `decpp` and `cmaescpp` which are 
slightly nore efficient. 

[source,python]
---- 
...
from scipy.optimize import Bounds
from fcmaes import de, cmaes, decpp, cmaescpp
import time
import ctypes as ct
import multiprocessing as mp 
from fcmaes.optimizer import logger, dtime
import math

class BaseProblem:

    def __init__(self):                
        self.evals = mp.RawValue(ct.c_long, 0)  # writable across python processes
        self.best_y = mp.RawValue(ct.c_double, math.inf) # writable across python processes
        self.t0 = time.perf_counter()

    def bounds(self):
        return Bounds(self.lbs(),self.ubs())

    def fun(self, xs):
        y = self.evaluate(xs)
        self.evals.value += 1
        if y < self.best_y.value:
            self.best_y.value = y            
            logger().info("evals = {0}: time = {1:.1f} y = {2:.5f} x= {3:s}"
                          .format(self.evals.value, dtime(self.t0), y, 
                                  '[' + ", ".join([f"{xi:.16f}" for xi in xs]) + ']'
                    ))
        return y
        
    def optimizeDE(self):
        self.bestY = 1E99
        self.bestX = []
        return de.minimize(self.fun, 
            dim = self.dims(),
            bounds = self.bounds(), 
            popsize = 24, 
            ints = [v != 'cont' for v in problem.vartype()],
            max_evaluations = 5000, 
            workers = 12,
        )

    def optimizeCMA(self):
        self.bestY = 1E99
        self.bestX = []
        return cma.minimize(self.fun, 
            bounds = self.bounds(), 
            popsize = 24, 
            max_evaluations = 5000, 
            workers = 12,
        )   
    ...

class DockerHospitalBenchmarkProblem(BaseProblem):

    def __init__(self, name, d, lbs, ubs, vartype, direction, errval):
        super().__init__()
        ...
        
    ...

if __name__ == '__main__':
    Hospital.optimizeDE()
    # Hospital.optimizeCMA()
----

After about four hours runtime (using a standard 16 core CPU AMD 5950x) we get a result 

- around 12.3 using DE (differential evolution) with `popsize = workers = 16`
- around 12.9 using CMA-ES with `popsize = workers = 16`

image::Hospital_Management_Optimization.png[]

When we search for published results for the hospital management problem we often find
relative but no absolute optimization results. Although
https://www.youtube.com/watch?v=Riio1eKOSKg&t=711s[Jan Hendrik Schön]  
proved that you can almost get a nobel price using relative results, 
see https://en.wikipedia.org/wiki/Sch%C3%B6n_scandal[Schön], this 
makes it difficult to evaluate the tested methods compared to other ones. 
Specifically to the 12.3 / 12.9 result we already have for parallel DE / CMA-ES.

An exception is: https://ir.cwi.nl/pub/31037/31037.pdf[Optimisation with a Random ReLU Expansion Surrogate Model] 

They report a result of their surrogate model based method of 

- 16.29 (+-2.16) and
- 14.81 (+-0.69) limiting the number of points suggested by the surrogate model

after only 200 evaluations. These results are very good, surpassing what we got from CMA-ES after only
200 evaluations. How do these methods scale when investing more evaluations? 
And even more important: How do they scale when investing more time considering parallelization?
All we know is that using the surrogate model based method you get better than random solutions quite fast.
As we do using differential evolution. 

=== Optimizing the CFD Electrostatic Precipitator problem

The CFD Electrostatic Precipitator problem can be found at 
https://github.com/AlgTUDelft/ExpensiveOptimBenchmark/blob/master/expensiveoptimbenchmark/problems/DockerCFDBenchmark.py[DockerCFDBenchmark.py],
see also https://algtudelft.github.io/ExpensiveOptimBenchmark/problems.html[Electrostatic Precipitator]
An Electrostatic Precipitator is a large gas filtering installation, whose efficiencyis dependent on how well the intake gas is distributed. This installation has slots which can be of various types, each having a different impact on the distribution. This benchmark problem employs the OpenFOAM Computational Fluid Dynamics simulator. The goal is to find a configuration that has the best resulting distribution. 

We executed the "ESP" benchmark with parallel differential evolution:

image::CFD_ESP_Optimization.png[]

==== UPDATE:

After the fcmaes DE algorithm got a "mixed integer" upgrade, we executed the experiment again. Since almost all ESP variables
are discrete, the result improved significantly:

image::DE_ESP_CFD_Optimization_problem.png[]

Parallel execution on an AMD 5950x CPU enabled an execution time
of about `1.4` sec / evaluation.

=== Optimizing a Windmill Wake

A windmill wake simulator based optimization using https://github.com/NREL/floris[floris]
see https://algtudelft.github.io/ExpensiveOptimBenchmark/problems.html[Windmill Wake Simulator], code is here:
https://github.com/AlgTUDelft/ExpensiveOptimBenchmark/blob/master/expensiveoptimbenchmark/problems/windwake.py[windwake.py]
The layout of the windmills in a wind farm has noticeable impact on the amount of energy it produces. 
This benchmark problem employs the FLORIS wake simulator to analyse how much power production is lost by
having windmills be located in each others wake. The objective is to maximize power production.

We executed the `WindWakeLayout('example_input.json', n_samples=5)` benchmark using https://raw.githubusercontent.com/NREL/floris/master/examples/example_input.json[example_input.json] with parallel differential evolution:

image::WindWake_Optimization.png[]

Parallel execution on an AMD 5950x CPU enabled an execution time
of about `0.1` sec / evaluation. 

=== Optimizing Hyper-Parameters

Hyper-parameter optimization using `xgboost` see https://algtudelft.github.io/ExpensiveOptimBenchmark/problems.html[HPO / XGBoost], code is here:
https://github.com/AlgTUDelft/ExpensiveOptimBenchmark/blob/master/expensiveoptimbenchmark/problems/hpo.py[hpo.py]
This benchmark makes use of scikit-learn to build an XGBoost classifier with per-feature preprocessing. Evaluation of a solution
is performed by k-fold cross validation, with the goal to maximize accuracy.

We executed the `HPOSFP` benchmark using the dataset provided by Semeion, Research Center of Sciences of Communication 
with parallel differential evolution:

image::Hyper_Parameter_Optimization2.png[]

==== UPDATE:

After the fcmaes DE algorithm got a "mixed integer" upgrade, we executed the experiment again. Since many HPO variables
are discrete, the result improved:

image::DE_Hyper_Parameter_Optimization_problem.png[]

Parallel execution on an AMD 5950x CPU enabled an execution time
of about `0.15` sec / evaluation.

In https://dl.acm.org/doi/10.1145/3449726.3463136[Bliek21] 
you may find results for surrogate based optimizers for this problem.
