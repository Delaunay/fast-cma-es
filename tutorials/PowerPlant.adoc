:encoding: utf-8
:imagesdir: img
:cpp: C++
:call: __call__

= Power Plant Efficiency

This tutorial

- Is related to https://tespy.readthedocs.io/en/main/tutorials_examples.html#thermal-power-plant-efficiency-optimization[power-plant-efficiency-optimization].
- Shows how to configure parallel optimization for complex simulations.

The code for this tutorial is
here: https://github.com/dietmarwo/fast-cma-es/blob/master/examples/powerplant.py[powerplant.py]

=== Motivation

https://github.com/oemof/tespy[tespy] provides a powerful tool to simulate thermal engineering systems. 
One purpose of such a simulation is the "tweaking" of parameters to improve some properties
of the system, for instance efficiency, cost or that certain conditions are fulfilled. 
There is a short 
https://tespy.readthedocs.io/en/main/tutorials_examples.html#thermal-power-plant-efficiency-optimization[tutorial] but 
only for a very simple system, a single objective (efficiency) and a trivial constraint.

But before we can handle more complex simulations optimizing multiple objectives and constraints
we first should analyze and test, which algorithms work in connection with https://github.com/oemof/tespy[tespy].
tespy is implemented using statics instead of singletons so we expect some issues regarding parallelization. 
Try to avoid statics if your code can be called in parallel, but also be careful with
https://skillenai.com/2020/12/05/singleton-fails-with-multiprocessing-in-python/[singletons in Python].
  
The original optimization tutorial uses https://esa.github.io/pygmo2/[pygmo] which supports
https://esa.github.io/pygmo2/archipelago.html[parallelization],
but this is implemented in {cpp}. I couldn't get pygmo parallelization working with tespy, 
may be it is because of the libraries tespy uses. fcmaes uses {cpp} multithreading
only for parallel function evaluation using the {cpp} variants of differential evolution and 
the multi-objective MODE algorithm. They don't work either with tespy, but fcmaes 
has additional Python implementations of these algorithms which work, as does parallel
optimization retry both with Python and {cpp} algorithms. 

=== Thermal Power Plant Efficiency
Note: tespy doesn't support yet Python 3.9, tested on anaconda with python 3.8 on linux. 
The power plant model used in 
https://github.com/dietmarwo/fast-cma-es/blob/master/examples/powerplant.py[powerplant.py]
can modify the pressure at two "extraction" connections, these pressures are the decision
variables we want to optimize. After the simulation we divide "power" and "heat" to 
determine the efficiency we want to maximize:

[source,python]
----
    def calculate_efficiency(self, x):
        # set extraction pressure
        self.nw.get_conn('extraction1').set_attr(p=x[0])
        self.nw.get_conn('extraction2').set_attr(p=x[1])

        self.nw.solve('design')
        ...
        return self.nw.busses['power'].P.val / self.nw.busses['heat'].P.val
----

For such a simple problem the https://esa.github.io/pygmo2/algorithms.html?highlight=ihs#pygmo.ihs[harmony search]
algorithm proposed by the original tutorial is sufficient. But as soon as there are
more parameters, objectives and constraints, better algorithms and faster function
evaluation (by parallelization) is necessary. 

=== CPU Efficiency

For this section we use a different CPU, a modern 8 core / 16 thread Intel laptop CPU. 
Reason is that the parallelization scaling issues we want to show are more prominent here.   

We will see the the configuration of the Python interpreter regarding
BLAS/MKL can have a dramatic effect (up to factor 100) regarding the tespy power plant
simulation. Lets run the pygmo harmony search optimization as proposed in the
original tutorial. We only did some minor changes to monitor the number of evaluations
executed per second. Lets execute:

[source,python]
----
    optimize_pygmo()
----

which optimizes with pygmo/pagmo harmony search without parallelization:

[source,python]
----
    pop = pg.population(prob, size=32)
    algo = pg.algorithm(pg.ihs(gen=10))
----

We get:

[source]
----
Evolution: 149 Evals 1522 time 188.68 evals/time 8.07
----

image::CPU96.png[]

This means we have 8.07 model.calculate_efficiency(x) calls per second.
Surprisingly we see 96 % CPU load, it should be 6% only (8 cores / 16 threads)
We suspect BLAS/MKL parallelization being the source of the issue. 

What happens if we restrict to 1 thread for BLAS?

[source,python]
----
            with threadpoolctl.threadpool_limits(limits=1, user_api="blas"):
                f1 = 1 / self.model.calculate_efficiency(x)
----
We get:

[source]
----
Evolution: 149 Evals 1522 time 97.61 evals/time 15.59
----

image::CPU6.png[]

We have 6% CPU load now and 15.59 calls per second. Quite a bad scaling, isn't it?
Actually giving blas what it wants reduces performance by factor 2. 
Ok, if it is winter, our CPU can serve as a room heater, but I doubt that 
is what parallelization is about. 

The dangerous thing is that at least for for anaconda the 
"room heating" mode is the default.  

But the worst is still to come: What happens if we apply parallel function evaluation
using fcmaes differential evolution? We configure 16 workers, since we are 
currently using a 8 core 16 threads intel CPU:
[source,python]
----

de.minimize(wrapper(problem.fitness_so), problem.dim, problem.bounds, max_evaluations = 10000, workers=16)
----
We see in the logs:

[source]
----
395.91 370 1.0 -0.448594894567987 [25.9229649142181, 2.7403404173179116]
----

This means 395 seconds for 370 evaluations. Ouch. We had 96% CPU load already 
for the non-parallel run. Now the CPU is hopelessly overloaded and the machine
becomes unresponsive. The additional parallelization at the optimizer level
slows down the computation by factor 8!

Lets modify the fcmaes fitness function:

[source,python]
----
def fitness(self, x):
    with threadpoolctl.threadpool_limits(limits=1, user_api="blas"):
            y = -self.efficiency(x)
----   
Now we get:

[source]
----
53.73 5365 100.0 -0.4485959420678582 [25.829292859306605, 2.6867346018603175]   
----
100 evaluations per second. This is almost factor 100 compared
to the previous experiment with only one additional line of code. 
Now we get at least something for the heat our CPU dissipates. 
Remark: fcmaes now uses "with threadpoolctl.threadpool_limits(limits=1, user_api="blas"):"
internally since version 1.3.32. The old behavior can no longer be reproduced easily. 
You would have to comment out the `threadpoolctl.threadpool_limits` line in fcmaes/evaluator.py. 

Takeways: 

- Check the CPU load for single threaded optimization. If it is high, try
the "with threadpoolctl.threadpool_limits(limits=1, user_api="blas"):" trick. 
- Parallel optimization only works if function evaluation is single threaded
- blas parallelization often hurts the performance even without parallelization at
  the optimizer level. 
- fcmaes used `os.environ['MKL_NUM_THREADS'] = '1'` to achieve the same, but this doesn't 
  work on all CPUs/python installations.
- fcmaes now uses "with threadpoolctl.threadpool_limits(limits=1, user_api="blas"):"
  internally for parallel retry since version 1.3.34. Because of its overhead
  it is not used for parallel function evaluation, so in this case make sure 
  you define it in your fitness function if necessary on your CPU.   

=== Applying fcmaes to optimize the power plant: 

In this tutorial we show how the fcmaes
algorithms can be applied. As for pygmo, we first wrap the fitness function into
a python class which collects all information necessary to perform the optimization
together with two fitness functions: One returning the constraint separately and
one adding a penalty for constraint violations.

[source,python]
----
    class fcmaes_problem():
        
        def __init__(self):
            self.dim = 2
            self.nobj = 1
            self.ncon = 1
            self.bounds = Bounds([1]*self.dim, [40]*self.dim)          
            self.local = threading.local()
           
        def get_model(self):
            if not hasattr(self.local, 'model'):
                self.create_model()
            return self.local.model
        
        def create_model(self):
            self.local.model = PowerPlant()
        
        def efficiency(self, x):   
            try:
                with threadpoolctl.threadpool_limits(limits=1, user_api="blas"):
                    eff = self.get_model().calculate_efficiency(x)      
                if not np.isfinite(eff): # model gets corrupted in case of an error
                    self.create_model() # we need to recreate the model
                    return 0
                return eff
            except Exception as ex:
                return 0  
  
        def fitness(self, x):
            y = -self.efficiency(x)
            c = -x[0] + x[1]
            return [y, c]
    
        def fitness_so(self, x):
            if x[1] > x[0]: # handle constraint
                return 1000 + x[1] - x[0]
            return -self.efficiency(x)
----

The constraint `c = -x[0] + x[1]` only requires the second pressure to be lower than the
first one, it can very easily be handled by a simple linear penalty. Advantage is, 
that we now have a much greater choice of algorithms: BiteOpt, Differential Evolution, 
CMA-ES, CR-FM-NES and others. The only fcmaes algorithm supporting explicit constraints
is MODE. There is no need to handle equality constraints separately, since they easily
can be converted into inequality constraints: a = b -> abs(a-b) <= 0. 

The lines:
[source,python]
----
    if not np.isfinite(eff): # model gets corrupted in case of an error
        self.create_model() # we need to recreate the model
----
recreates the model in case of an error. We noticed, that the model produced values
"too good to be true" after this happened. This problem can be reproduced even with
non-parallel optimization. We filed a bug for tespy regarding this issue, but 
until it is solved, we need this workaround. Note that we use thread local model instances
to avoid multi-threading issues. 
To perform an experiment you have to run
[source,python]
----
optimize_fcmaes()
----
after uncommenting one of the optimizer calls: 

[source,python]
----

    # Parallel retry of different single-objective optimizers

    # ret = retry.minimize(wrapper(problem.fitness_so), problem.bounds,
    #                       num_retries = 32, optimizer=Bite_cpp(20000))            
    #
    # ret = retry.minimize(wrapper(problem.fitness_so), problem.bounds,
    #                       num_retries = 32, optimizer=De_cpp(20000))     
    #
    # ret = retry.minimize(wrapper(problem.fitness_so), problem.bounds,
    #                       num_retries = 32, optimizer=Cma_cpp(20000))       
    #
    # ret = retry.minimize(wrapper(problem.fitness_so), problem.bounds,
    #                       num_retries = 32, optimizer=Crfmnes_cpp(20000))          
   
    # Multi objective optimization parallel retry:   
 
    # x, y = modecpp.retry(mode.wrapper(problem.fitness, problem.nobj), problem.nobj, 
    #              problem.ncon, problem.bounds, 
    #              popsize = 32, max_evaluations = 1000000, 
    #              nsga_update=True, num_retries = 32,
    #              workers=32)
    #
    # # Differential Evolution using parallel function evaluation:
    #
    ret = de.minimize(wrapper(problem.fitness_so), problem.dim, problem.bounds, max_evaluations = 20000, workers=32)   
    
    # Multi objective optimization using parallel function evaluation:         

    # x, y = mode.minimize(mode.wrapper(problem.fitness, problem.nobj), problem.nobj, 
    #                            problem.ncon, problem.bounds, 
    #                            popsize = 32, max_evaluations = 100000, nsga_update=True, 
    #                            workers=32)

    # The C++ version of this algorithm only works single threaded with tespy, but modecpp.retry works multi threaded 
        
    # x, y = modecpp.minimize(mode.wrapper(problem.fitness, problem.nobj), problem.nobj, 
    #                            problem.ncon, problem.bounds, 
    #                            popsize = 32, max_evaluations = 100000, nsga_update=True, 
    #                            workers=1)
       
    # some single threaded single objective optimizers
          
    #ret = decpp.minimize(wrapper(problem.fitness_so), problem.dim, problem.bounds, max_evaluations = 20000)            
    
    #ret = cmaes.minimize(wrapper(problem.fitness_so), problem.bounds, max_evaluations = 20000)            
    
    #ret = bitecpp.minimize(wrapper(problem.fitness_so), problem.bounds, max_evaluations = 20000)            
    
    #ret = de_cma(20000).minimize(wrapper(problem.fitness_so), problem.bounds)    
----

Preconfigured is 
[source,python]
----
de.minimize(wrapper(problem.fitness_so), problem.dim, problem.bounds, max_evaluations = 20000, workers=32) 
----
which executes the fcmaes Differential Evolution algorithm performing parallel function evaluations. 

On an AMD 5950x 16 core CPU we see something like:
[source]
----
39.15 13535 346.0 -0.4485959202134408 [25.829239333756185, 2.686719511836477]
---- 
which means:

- time = 39.15 seconds
- evaluations = 13535
- 346 evaluations / second
- -0.4485959202134408 actual efficiency (negative because we maximize and fcmaes minimizes)
- [25.829239333756185, 2.686719511836477] the configured pressure levels. 

Single threaded we get about 18-19 evaluations per second, so we see a nice scaling with the number
of CPU scores utilized: 

image::CPU100.png[]
 
==== Conclusion

- fcmaes is a good choice for expensive Python simulations such as https://github.com/oemof/tespy[tespy] models.
- It supports parallel function evaluations and parallel optimization retries.
- Modern CPUs scale well with the number of cores utilized - as long as the fitness function is single threaded
- BLAS usage needs to be restricted to one thread, which in case of https://github.com/oemof/tespy[tespy] surprisingly
also increases performance if the simulation is not called in parallel.
- Use thread local model instances to avoid parallelization issues.
- Errors during the simulation can corrupt the model. We mitigate this tespy bug by recreating the model if an error occurs.  

