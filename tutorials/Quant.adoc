:encoding: utf-8
:imagesdir: img
:cpp: C++
:call: __call__

= fcmaes - a Python 3 gradient-free optimization library

https://gitter.im/fast-cma-es/community[image:https://badges.gitter.im/Join%20Chat.svg[]]

image::logo.gif[]

== Optimization of a Variational Qubit

This tutorial

- Shows how to optimize a variational quantum-bit (qubit) adapting
  https://qiskit.org/textbook/ch-applications/vqe-molecules.html#Example-with-a-Single-Qubit-Variational-Form[Example-with-a-Single-Qubit-Variational-Form]
- Compares results of three fcmaes algorithms (DE, CMA-ES and BiteOpt) to the one proposed by IBM (COBYLA). 
- Applies different parallelization methods for optimizations using the the "qasm_simulator" from  https://qiskit.org/[Quiskit]. 

The code for this tutorial is here: 

- https://github.com/dietmarwo/fast-cma-es/blob/master/examples/quant.py[quant.py].

== How does a Quantum Computer work ? 

A quantum computer combines the precision of a digital with the expressive power of an analog computer. 

https://qiskit.org/textbook[textbook] and http://mmrc.amss.cas.cn/tlb/201702/W020170224608150507023.pdf[introduction] provide 
good tutorials you may use to understand the details. Essentially it works as follows:
 
 - Conventional bits are transformed into qubits.
 - A quantum algorithm is a network of connected qubit gates executed using a simulator or a real quantum computer. 
 - The resulting qubits are transformed back into conventional bits. 
 

A qubit can be represented by a 3-dimensional unit-vector. Qubit
gates rotate this vector in different directions. You may play with the colored buttons on the 
https://javafxpert.github.io/grok-bloch/[bloch sphere] to see how these operations work. 

image::bloch.png[]

A qubit represents a probability distribution and a "phase" where
only the former can be observed. If you check the state of a qubit, it collapses into a conventional bit
according to the probability distribution. 

The expressive power of a sequence of qubits is based on the following facts:

- A single qubit corresponds to a continuous probability and phase. 
- N qubits represent 2^N probabilities - one for each bit-combination it can collapse into.  

How do we make sure that the final state of the qubits, after converting them back into conventional
bits, represents something meaningful? There are techniques like quantum fourier transformation and 
amplitude amplification which single out the desired bit combination in the final probability distribution.  

=== Can optimization related to quantum algorithms be parallelized ?

All qubit gates are rotations of a 3-dimensional unit vector. Some use fixed angles, some are configurable. 
Optimization often use these gate parameters which are configurable rotation angles as continuous decision variables. 
Since evaluation of the objective function involves quantum operations parallelization makes 
currently no sense if the "backend" used to execute these operations is a real quantum computer. Computing resources
on such a backend is still expensive, although this may change in the future. 

But there exist other backends for development and testing, which simulate quantum gates using conventional CPUs and GPUs. 
In this tutorial we will check using a concrete example both timing and quality of the optimization results when
we apply parallelization to optimize the qubit-parameters using a simulation-backend. But since this example
is quite small - involving a single qubit - we want to check quiskit's performance for a bigger benchmark: 
10 runs of a 12 qubit inverse fourier transform (see https://gist.github.com/dietmarwo/23d30a89018d62c02294525092093671[gist] )
We use a 16 core AMD 5950x CPU, a kind of "mainstream" modern many core CPU currently at around 500$. 
On this CPU we can expect about:

- factor 6-8 scaling for parallel objective function evaluation.
- factor 12-16 scaling for parallel optimization runs. 

As GPU we used a NVIDIA GTX 1660 Ti.

For both optimization parallelization options we have to switch off any simulation parallelism
(no device='GPU', max_parallel_threads=1) so it makes only sense to use them if the simulation
scales worse:

.Simulation benchmark - 10 runs of 12 qubit inverse fourier transform
[width="50%",options="header"]
|===
|simulator |options |time for 10 runs in sec
|aer_simulator|none|2.11
|aer_simulator|max_parallel_threads=1|1.82
|aer_simulator|device='GPU'|1.56
|aer_simulator_density_matrix|none|10.06
|aer_simulator_density_matrix|max_parallel_threads=1|34.15
|aer_simulator_density_matrix|device='GPU'|4.01
|aer_simulator_statevector|none|1.58
|aer_simulator_statevector|max_parallel_threads=1|1.6
|aer_simulator_statevector|device='GPU'|1.56
|qasm_simulator|none|1.60
|qasm_simulator|max_parallel_threads=1|1.60
|qasm_simulator|device='GPU'|1.56
|===

- aer_simulator, qasm_simulator and aer_simulator_statevector all don't scale with multi-threading
- For aer_simulator_density_matrix a GPU should be used if there are > 10 qubits. 
- For aer_simulator_density_matrix the picture changes if we reduce the number of qubits to 8:
  Now single threaded execution is faster than multi threaded and GPU usage looses its advantage
- For aer_simulator_density_matrix even with max_parallel_threads=1 you see many threads active 
  using the system monitor.

.Simulation benchmark - 10 runs of 8 qubit inverse fourier transform
[width="50%",options="header"]
|===
|simulator |options |time for 10 runs in sec
|aer_simulator_density_matrix|none|4.61
|aer_simulator_density_matrix|max_parallel_threads=1|0.94
|aer_simulator_density_matrix|device='GPU'|0.88
|===

These results mean, that for all quiskit simulators beside aer_simulator_density_matrix,
it makes sense to switch off parallelism (no device='GPU', max_parallel_threads=1) and use
fcmaes parallelism instead for optimizations. aer_simulator_density_matrix requires a lot
of memory which can prevent parallel execution even if no GPU is available. But if there are
less than 10 qubits even for aer_simulator_density_matrix it makes sense to switch off parallelism.

=== Single Qubit Variational Form

The problem solved by https://qiskit.org/textbook/ch-applications/vqe-molecules.html#Example-with-a-Single-Qubit-Variational-Form[Example-with-a-Single-Qubit-Variational-Form] is similar to ground state energy estimation, it determines the parameterization for a single qubit variational form such that it outputs a probability distribution that is close to some random target distribution. 
The complete example code can be found at https://github.com/dietmarwo/fast-cma-es/blob/master/examples/quant.py[quant.py]. 

The `objective_function` is only slightly adapted from the https://qiskit.org/textbook[quiskit textbook]. It uses `qiskit` operations and the "qasm_simulator" backend to determine the distance to the random target distribution `target_distr`.   

==== Objective Function

[source,python]
----  
def objective_function(params, target_distr):
    # Obtain a quantum circuit instance from the parameters
    qc = get_var_form(params)
    # Execute the quantum circuit to obtain the probability distribution associated with the current parameters
    t_qc = transpile(qc, backend)
    qobj = assemble(t_qc, shots=NUM_SHOTS)
    result = backend.run(qobj).result()
    # Obtain the counts for each measured state, and convert those counts into a probability vector
    output_distr = get_probability_distribution(result.get_counts(qc))
    # Calculate the cost as the distance between the output distribution and the target distribution
    cost = sum([np.abs(output_distr[i] - target_distr[i]) for i in range(2)])
    return cost
----

It is wrapped into a callable `Fitness`-object storing the target distribution.  

[source,python]
----
class Fitness(object):
    
    def __init__(self, target_distr):
        self.target_distr = target_distr
        self.bounds = Bounds([0]*3, [2]*3)      
        
    def __call__(self, x):  
        return objective_function(x, self.target_distr)
----

==== Comparison of Different Optmization Algorithms

All optimizers are given the same random target distributions generated
in advance, so that the results are comparable. 

[source,python]
----   
    # generate Fitness objects associated to random target distributions
    fits = [Fitness(random_target_distr()) for i in range(10)]
    opt_differential_evolution_loop(fits)
    opt_cmaes_loop(fits)
    opt_biteopt_loop(fits)
    opt_COBYLA_evolution_loop(fits)
----

All fcmaes optimizers are configured to use 16 parallel threads, COBYLA is single threaded. 
If you have a modern many-core CPU available you may reproduce the results by executing 
https://github.com/dietmarwo/fast-cma-es/blob/master/examples/quant.py[quant.py]. 
We used a 16 core AMD 5950x CPU / NVIDIA GTX 1660 Ti GPU for our tests. 

Read https://qiskit.org/documentation/getting_started.html[getting_started] about setting up 
your Python environment. You need to do:

[source,python]
---- 
    pip install qiskit
----

'pip install qiskit-aer-gpu' (GPU support) is not required for executing ' quant.py' - and doesn't work on AMD GPUs. 

==== COBYLA using no Parallelism

The COBYLA optimization was taken from
https://qiskit.org/textbook/ch-applications/vqe-molecules.html#Example-with-a-Single-Qubit-Variational-Form[Example-with-a-Single-Qubit-Variational-Form] and serves as a reference point. We increased `maxiter` to rule out that limit of the number of
iterations is a problem here. 


[source,python]
---- 
def opt_COBYLA_evolution_loop(fits):
    for fit in fits:
        params = np.random.rand(3)
        optimizer = COBYLA(maxiter=50000, tol=0.0001)
        ret = optimizer.minimize(fun=fit, x0=params)
----

COBYLA optimization is fast, it does apply far less than `maxiter` iterations. 
But the results are terribly bad, it seems COBYLA is not able to solve this quite 
easy 3-dimensional optimization problem reliably: 

[source,python]
----   
    COBYLA time 0.6 distance 0.004723912057785329
    COBYLA time 1.22 distance 0.09254840670649922
    COBYLA time 1.79 distance 0.5775480074342264
    COBYLA time 2.35 distance 1.1746999540117542
    COBYLA time 2.86 distance 0.2301463621426788
    COBYLA time 3.32 distance 0.043142750403738134
    COBYLA time 3.86 distance 0.15785308878979398
    COBYLA time 4.47 distance 0.015941335709322213
    COBYLA time 4.95 distance 0.015568947833576152
    COBYLA time 5.45 distance 1.2604080177937873
    
    COBYLA mean distance = 0.3572580782883162
    COBYLA std distance = 0.45984856126261725
----

Question is if you want to trust this algorithm for optimizing more complex quantum algorithms.
qiskit maps COBYLA to its scipy implementation. scipy offers more reliable alternatives
like differential evolution, but there is always a tradeoff between "reliability' and speed of
convergence - which fcmaes tries to mitigate by supporting parallelism. 

==== Differential Evolution using Parallel Fitness Evaluation 

fcmaes offers a different variant of differential evolution compared to scipy, 
tuned for fast convergence and multiple parallel retrys. 
In this case it is configured to perform parallel function evaluation
instead (*workers = 16'):

[source,python]
----  
    def opt_differential_evolution_loop(fits):
        ...
        for fit in fits: 
            ret = de.minimize(fit, 3, fit.bounds, max_evaluations = 1000, 
                              stop_fitness = 0.00001, workers=16)
            ...
----

It uses all 1000 configured fitness evaluation, which means it is still slower than COBYLA,
but on the other hand very reliable: 

[source,python]
----   
    de time 1.08 distance 7.608794221475312e-05
    de time 2.15 distance 5.159329350079567e-05
    de time 3.23 distance 0.00025199256577354556
    de time 4.31 distance 9.995401175424967e-05
    de time 5.38 distance 5.363785732115378e-05
    de time 6.45 distance 0.00025724959626183264
    de time 7.53 distance 5.30887897939869e-05
    de time 8.62 distance 0.00014133570932223227
    de time 9.71 distance 3.105216642390607e-05
    de time 10.81 distance 0.00020801779378730456
    
    de mean distance = 0.00012240097261537604
    de std distance = 8.258724841147236e-05
----

Note that we didn't use the alternative C++ implementation of DE fcmaes offers, because
parallel function evaluation is slower in this specific application context.  

==== CMA-ES using Parallel Fitness Evaluation 

The fcmaes CMA-ES implementation also offers parallel fitness evaluation, which we use here:

[source,python]
----  
    def opt_differential_evolution_loop(fits):
        ...
        for fit in fits: 
            ret = cmaes.minimize(fit, fit.bounds, input_sigma=0.7, 
                        max_evaluations = 1000, stop_fitness = 0.00001, workers=16)            ...
----

The results are similar to the one for fcmaes-DE, but slightly worse:

[source,python]
----   
    cmaes time 0.51 distance 0.026598245672092756
    cmaes time 1.55 distance 0.004407006710436312
    cmaes time 2.59 distance 7.25900467107854e-05
    cmaes time 3.65 distance 0.0001793522710383244
    cmaes time 4.69 distance 0.00016477295389366597
    cmaes time 5.78 distance 9.85274772162259e-05
    cmaes time 6.83 distance 0.00011994884350791102
    cmaes time 7.87 distance 5.438697928394909e-05
    cmaes time 8.92 distance 4.5451310954014446e-05
    cmaes time 9.98 distance 0.00014988796844872532
    
    cmaes mean distance = 0.003189017023358267
    cmaes std distance = 0.007907630855455892
----


==== BiteOpt using Parallel Optimization Retry 

https://github.com/avaneev/biteopt[BiteOpt] is written in C++ and doesn't support parallel
fitness evaluation as the two algorithms before. It is a very good choice if applied single threaded
or in the context of multiple parallel optimization retries. This approach sacrifices performance
for reliability. 

[source,python]
----  
def opt_biteopt_loop(fits):
    ...
    for fit in fits:  
        ret = retry.minimize(fit, fit.bounds, logger = None, 
                              num_retries=16, optimizer=Bite_cpp(100), workers=16)
    ...
----

The results are worse than the ones for differential evolution above, but BiteOpt may turn out
superior for harder optimization problems / larger quantum algorithms. It also is an excellent choice when applied
single threaded for multi-threaded/GPU simulations, then multiple sequential retries may be required
to obtain a reliable result. 

[source,python]
----   
    bite time 1.96 distance 7.608794221475312e-05
    bite time 3.85 distance 0.0001484067064991823
    bite time 5.76 distance 0.00145199256577358
    bite time 7.65 distance 9.995401175424967e-05
    bite time 9.57 distance 0.0001463621426788242
    bite time 11.48 distance 5.724959626185466e-05
    bite time 13.38 distance 0.0002530887897940204
    bite time 15.23 distance 0.00014133570932223227
    bite time 17.18 distance 3.105216642390607e-05
    bite time 19.11 distance 0.0009919822062127437
    
    bite mean distance = 0.0003397511836935346
    bite std distance = 0.000456672806132943
----

==== Search for Weaknesses of Optimization Algorithms

Finally we apply parallel optimization to search for weaknesses of COBYLA.
This means the fitness function performs a COBYLA optimization and searches
for a target distribution and an initial guess maximizing the final distance 
COBYLA returns:  

[source,python]
----  
    def find_COBYLA_weakness():
        
        def fitness(x):
            params = x[:3] # use first three decision variables as guess for COBYLA
            target_distr =  x[3:] # use two decision variables as target
            ...     
            fit = Fitness(target_distr)
            ret = COBYLA(maxiter=50000, tol=0.00001).minimize(fun=fit, x0=params)
            return -ret.fun # we maximize the distance
            
        bounds = Bounds([0]*5, [2]*3 + [1]*2)
        ret = de.minimize(wrapper(fitness), 5, bounds, max_evaluations = 300, workers=16)
        print("worst COBYLA distance = " +  str(ret.fun))
----

We get

[source,python]
----
0.61 16 26.0 -0.16048111250104818 [1.5150607763293302, 0.855535102710067, 0.9443911196710082, 0.44315944374947586, 0.5568405562505241]
0.71 19 27.0 -0.3451956010096087 [1.2667012355920217, 0.6074735090610092, 0.0, 0.5760978005048043, 0.4239021994951956]
0.72 20 28.0 -0.39053196482825997 [1.819039398258975, 0.5500857586958143, 0.473575221093664, 0.39823401758587, 0.6017659824141299]
0.74 21 28.0 -0.4871209992729757 [1.817252059960992, 0.0, 0.43090691832695105, 0.6197604996364878, 0.3802395003635122]
1.38 33 24.0 -1.5241888666705452 [2.0, 0.9968046393254453, 0.6468567010495877, 0.7675944333352724, 0.2324055666647275]
2.94 70 24.0 -1.7722 [2.0, 1.164716055769154, 0.8828197082450224, 1.0, 0.0]
5.14 117 23.0 -1.7860626205507981 [2.0, 1.260050169662025, 1.1363082378155278, 0.996531310275399, 0.003468689724600905]
7.27 163 22.0 -1.8356 [2.0, 1.5650815562087286, 0.6690050055417369, 1.0, 0.0]
8.69 197 23.0 -1.9714 [1.9104317884199735, 1.7238262098841646, 0.6395695514606753, 1.0, 0.0]
worst COBYLA distance = 1.9714
----

confirming that we can get bad optimization results using COBYLA.

=== Conclusion

- The COBYLA algorithm proposed in https://qiskit.org/textbook/ch-applications/vqe-molecules.html#Example-with-a-Single-Qubit-Variational-Form[Example-with-a-Single-Qubit-Variational-Form] is quite unreliable even for a 3-dimensional single qubit related optimization problem. 
- Alternatives adding reliability may converge slower.
- To mitigate this parallel optimization or parallel fitness evaluation can be used. 
- All three fcmaes-algorithms tested work reliably for the single qubit variational form optimization problem.
- Differential Evolution with parallel fitness evaluation offers the best time / reliability compromise.
- For bigger quantum algorithms "qasm_simulator", "aer_simulator_statevector", and - to a lesser degree - "aer_simulator"
  work well with multithreading/GPU switched off, so we can profit from the scaling parallel optimization provides. 
- BiteOpt with parallel optimization retry is a very reliable option for more complex quantum related optimizations. 
