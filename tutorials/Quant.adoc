:encoding: utf-8
:imagesdir: img
:cpp: C++
:call: __call__

= fcmaes - a Python 3 gradient-free optimization library

https://gitter.im/fast-cma-es/community[image:https://badges.gitter.im/Join%20Chat.svg[]]

image::logo.gif[]

== Optimization of a Variational Qubit

This tutorial

- Shows how to optimize a variational quantum-bit (qubit) adapting
  https://qiskit.org/textbook/ch-applications/vqe-molecules.html#Example-with-a-Single-Qubit-Variational-Form[Example-with-a-Single-Qubit-Variational-Form]
- Compares results of three fcmaes algorithms (DE, CMA-ES and BiteOpt) to the one proposed by IBM (COBYLA). 
- Applies different parallelization methods for optimizations using the the "qasm_simulator" from  https://qiskit.org/[Quiskit]. 

The code for this part of the tutorial is here: 

- https://github.com/dietmarwo/fast-cma-es/blob/master/examples/quant.py[quant.py].

Note that the optimization methods shown can easily be transferred to a vendor-independent 
quantum infrastructure like https://github.com/qcc4cp/qcc[qcc] . 

== Optimization of a VQE (Variational Quantum Eigensolver)

In a second part we analyze whether what we learned from the first part is applicable to a 
bigger experiment: The optimization of a VQE to solve the maxcut graph problem. 

- Adapts https://qiskit.org/documentation/optimization/tutorials/06_examples_max_cut_and_tsp.html[06_examples_max_cut_and_tsp.html] to check
  parallel optimization both using fcmaes and qiskit optimizers. 
- Compares results for different simulation backends.
- What happens if we add noise ?

The code for this part of the tutorial is here: 

- https://github.com/dietmarwo/fast-cma-es/blob/master/examples/maxcut.py[maxcut.py]. 

=== How does a Quantum Computer work ? 

A quantum computer combines the precision of a digital with the expressive power of an analog computer. 
https://youtu.be/2Eswqed8agg[Quantum programming], https://qiskit.org/textbook[textbook] and http://mmrc.amss.cas.cn/tlb/201702/W020170224608150507023.pdf[introduction] provide 
good tutorials you may use to understand the details. Essentially it works as follows:
 
 - Conventional bits are transformed into qubits.
 - A quantum algorithm is a network of connected qubit gates executed using a simulator or a real quantum computer. 
 - The resulting qubits are transformed back into conventional bits.

A qubit can be represented by a 3-dimensional unit-vector. Qubit
gates rotate this vector in different directions. You may play with the colored buttons on the 
https://javafxpert.github.io/grok-bloch/[bloch sphere] to see how these operations work. 
Control gates correlate multiple qubits, this correlation is called "entanglement". This means
a "target" qubit rotates (changes probability and phase) dependent on a "control" qubit -
both qubits are no longer independent.    

image::bloch.png[]

A qubit represents a probability distribution and a "phase" where
only the former can be observed. If you check the state of a qubit, it collapses into a conventional bit
according to the probability distribution. 

The expressive power of a sequence of qubits is based on the following facts:

- A single qubit corresponds to a continuous probability and phase. 
- N qubits represent 2^N probabilities - one for each bit-combination it can collapse into.  

How do we make sure that the final state of the qubits, after converting them back into conventional
bits, represents something meaningful? There are techniques like quantum fourier transform and 
amplitude amplification based on phase resonance which single out the desired bit combination 
in the final probability distribution.  

=== Quantum Optimization

The basis of quantum optimization is a "variational algorithm" which vary a quantum circuit 
to obtain approximate solutions to an optimization problem.
From the perspective of classical continuous optimization this means that we implement our fitness function
using a parameterizable quantum algorithm. This makes most sense in the realm of physics, because
nature itself "is quantum". See 
https://physicsworld.com/a/conquering-the-challenge-of-quantum-optimization/[challenge of quantum optimization]. 
Simulating real quantum effects is time consuming, so things can be speed up using the "real thing" inside
the fitness function. 

There is a myriad of papers on the topic “quantum approximate optimization algorithm” (QAOA)
because it can be applied partly already with the flawed quantum computers we have today.  
But https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.127.120502 and https://www.nature.com/articles/s41467-018-07090-4
report "barren plateaus" and theoretical limits of QAOA applied to other domains. Theoretical limits also apply 
when we train a deep neural network - which is a similar idea to place "something parameterizable" in between 
our decision variables and the final output. Nevertheless these manage to do amazing stuff. 
But we have to be careful when "transferring" our experience optimizing DNN parameters/weights to quantum circuits. 
Where in the DNN domain we can get away with relatively simple optimization methods derived from gradient decent,
this may not be the case in the quantum domain. As we will see below, even optimizing a trivial 1-qubit circuit
is not as easy as expected. 

Why is it interesting to optimize using a quantum simulation based on classical computation? Current quantum computers
often restrict the "wiring" of their qubits incompatible with the requirements of our quantum algorithm design. 
Instead of waiting for future quantum computers supporting more qubits and a more flexible wiring 
we can use a simulation today to measure how efficient our method will be in the future.  

=== Can optimization related to quantum algorithms be parallelized ?

All qubit gates are rotations of a 3-dimensional unit vector. Some use fixed angles, some are configurable. 
Optimization often use these gate parameters which are configurable rotation angles as continuous decision variables. 
Since evaluation of the objective function involves quantum operations parallelization makes 
currently no sense if the "backend" used to execute these operations is a real quantum computer. Computing resources
on such a backend is still expensive, although this may change in the future. 

But there exist other backends for development and testing, which simulate quantum gates using conventional CPUs and GPUs. 
In this tutorial we will check using a concrete example both timing and quality of the optimization results when
we apply parallelization to optimize the qubit-parameters using a simulation-backend. But since this example
is quite small - involving a single qubit - we want to check quiskit's performance for a bigger benchmark: 
10 runs of a 8-36 qubit inverse fourier transform (see https://gist.github.com/dietmarwo/23d30a89018d62c02294525092093671[gist] )
We use a 16 core AMD 5950x CPU, a kind of "mainstream" modern many core CPU currently at around 500$. 
On this CPU we can expect about:

- factor 6-8 scaling for parallel objective function evaluation.
- factor 12-16 scaling for parallel optimization runs. 

As GPU we used a NVIDIA GTX 1660 Ti.

For both optimization parallelization options we have to switch off any simulation parallelism
(no device='GPU', max_parallel_threads=1) so it makes only sense to use them if the simulation
scales worse:

- aer_simulator_density_matrix cannot handle 18 or more qubits on our machine, 
- aer_simulator_statevector fails at 30 qbits (insufficient memory). 

.Simulation benchmark - 10 runs of inverse fourier transform, time in sec
[width="50%",options="header"]
|===
|simulator |options |time 8 qbits|time 12 qbits|time 18 qbits|time 24 qbits|time 30 qbits|time 36 qbits
|aer_simulator|none|0.90|2.11|3.43|28.25|1427.3|14.14
|aer_simulator|max_parallel_threads=1|0.91|1.82|4.28|111.28|9035.0|12.46
|aer_simulator|device='GPU'|0.87|1.56|3.45|19.7|cuda error|13.89
|qasm_simulator|none|0.89|1.60|2.93|29.12|1434.6|14.38
|qasm_simulator|max_parallel_threads=1|0.90|1.60|4.09|110.66|9028.2|13.02
|qasm_simulator|device='GPU'|0.87|1.56|3.14|19.83|cuda error|14.49
|aer_simulator_statevector|none|0.91|1.58|3.61|28.85|1430.8|-
|aer_simulator_statevector|max_parallel_threads=1|0.89|1.6|3.88|110.4|9022.1|-
|aer_simulator_statevector|device='GPU'|0.87|1.56|2.96|19.31|cuda error|-
|aer_simulator_density_matrix|none|0.91|10.06|-|-|-|-
|aer_simulator_density_matrix|max_parallel_threads=1|0.89|34.15|-|-|-|-
|aer_simulator_density_matrix|device='GPU'|0.87|4.01|-|-|-|-
|===

Summarizing the results:

- For a small number of qbits (<= 18) aer_simulator, qasm_simulator and aer_simulator_statevector don't scale significantly, neither with multi-threading nor by using a GPU. 
- For 8 qubits this also is true for aer_simulator_density_matrix, for 12 qubits factor 3.5 can be achieved using multi threading and factor 8.5 by using a GPU. 
- For 24 and 30 qubits we see for aer_simulator, qasm_simulator and aer_simulator_statevector factor 3.5 using multi threading and factor 5.5 using a GPU. 
- For 36 qubits stranglely we again don't see significant scaling, maybe internally another simulation method is used. 

These results mean, that if we don't use aer_simulator_density_matrix and our quantum circuit is relatively small
(<= 18 qbits) we can switch off parallelism and utilize optimization parallelism. For more than 18 qubits
we should benchmark our circuit to check how it scales. 

=== Single Qubit Variational Form

The problem solved by https://qiskit.org/textbook/ch-applications/vqe-molecules.html#Example-with-a-Single-Qubit-Variational-Form[Example-with-a-Single-Qubit-Variational-Form] is similar to ground state energy estimation, it determines the parameterization for a single qubit variational form such that it outputs a probability distribution that is close to some random target distribution. 
The complete example code can be found at https://github.com/dietmarwo/fast-cma-es/blob/master/examples/quant.py[quant.py]. 

The `objective_function` is only slightly adapted from the https://qiskit.org/textbook[quiskit textbook]. It uses `qiskit` operations and the "qasm_simulator" backend to determine the distance to the random target distribution `target_distr`.   

==== Objective Function

[source,python]
----  
def objective_function(params, target_distr):
    # Obtain a quantum circuit instance from the parameters
    qc = get_var_form(params)
    # Execute the quantum circuit to obtain the probability distribution associated with the current parameters
    t_qc = transpile(qc, backend)
    qobj = assemble(t_qc, shots=NUM_SHOTS)
    result = backend.run(qobj).result()
    # Obtain the counts for each measured state, and convert those counts into a probability vector
    output_distr = get_probability_distribution(result.get_counts(qc))
    # Calculate the cost as the distance between the output distribution and the target distribution
    cost = sum([np.abs(output_distr[i] - target_distr[i]) for i in range(2)])
    return cost
----

It is wrapped into a callable `Fitness`-object storing the target distribution.  

[source,python]
----
class Fitness(object):
    
    def __init__(self, target_distr):
        self.target_distr = target_distr
        self.bounds = Bounds([0]*3, [2]*3)      
        
    def __call__(self, x):  
        return objective_function(x, self.target_distr)
----

==== Comparison of Different Optmization Algorithms

All optimizers are given the same random target distributions generated
in advance, so that the results are comparable. 

[source,python]
----   
    # generate Fitness objects associated to random target distributions
    fits = [Fitness(random_target_distr()) for i in range(10)]
    opt_differential_evolution_loop(fits)
    opt_cmaes_loop(fits)
    opt_biteopt_loop(fits)
    opt_COBYLA_evolution_loop(fits)
----

All fcmaes optimizers are configured to use 16 parallel threads, COBYLA is single threaded. 
If you have a modern many-core CPU available you may reproduce the results by executing 
https://github.com/dietmarwo/fast-cma-es/blob/master/examples/quant.py[quant.py]. 
We used a 16 core AMD 5950x CPU / NVIDIA GTX 1660 Ti GPU for our tests. 

Read https://qiskit.org/documentation/getting_started.html[getting_started] about setting up 
your Python environment. You need to do:

[source,python]
---- 
    pip install qiskit
----

'pip install qiskit-aer-gpu' (GPU support) is not required for executing ' quant.py' - and doesn't work on AMD GPUs. 

==== COBYLA using no Parallelism

The COBYLA optimization was taken from
https://qiskit.org/textbook/ch-applications/vqe-molecules.html#Example-with-a-Single-Qubit-Variational-Form[Example-with-a-Single-Qubit-Variational-Form] and serves as a reference point. We increased `maxiter` to rule out that limit of the number of
iterations is a problem here. 


[source,python]
---- 
def opt_COBYLA_evolution_loop(fits):
    for fit in fits:
        params = np.random.rand(3)
        optimizer = COBYLA(maxiter=50000, tol=0.0001)
        ret = optimizer.minimize(fun=fit, x0=params)
----

COBYLA optimization is fast, it does apply far less than `maxiter` iterations. 
But the results are terribly bad, it seems COBYLA is not able to solve this quite 
easy 3-dimensional optimization problem reliably: 

[source,python]
----   
    COBYLA time 0.6 distance 0.004723912057785329
    COBYLA time 1.22 distance 0.09254840670649922
    COBYLA time 1.79 distance 0.5775480074342264
    COBYLA time 2.35 distance 1.1746999540117542
    COBYLA time 2.86 distance 0.2301463621426788
    COBYLA time 3.32 distance 0.043142750403738134
    COBYLA time 3.86 distance 0.15785308878979398
    COBYLA time 4.47 distance 0.015941335709322213
    COBYLA time 4.95 distance 0.015568947833576152
    COBYLA time 5.45 distance 1.2604080177937873
    
    COBYLA mean distance = 0.3572580782883162
    COBYLA std distance = 0.45984856126261725
----

Question is if you want to trust this algorithm for optimizing more complex quantum algorithms.
qiskit maps COBYLA to its scipy implementation. scipy offers more reliable alternatives
like differential evolution, but there is always a tradeoff between "reliability' and speed of
convergence - which fcmaes tries to mitigate by supporting parallelism. 

==== Differential Evolution using Parallel Fitness Evaluation 

fcmaes offers a different variant of differential evolution compared to scipy, 
tuned for fast convergence and multiple parallel retrys. 
In this case it is configured to perform parallel function evaluation
instead (*workers = 16'):

[source,python]
----  
    def opt_differential_evolution_loop(fits):
        ...
        for fit in fits: 
            ret = de.minimize(fit, 3, fit.bounds, max_evaluations = 1000, 
                              stop_fitness = 0.00001, workers=16)
            ...
----

It uses all 1000 configured fitness evaluation, which means it is still slower than COBYLA,
but on the other hand very reliable: 

[source,python]
----   
    de time 1.08 distance 7.608794221475312e-05
    de time 2.15 distance 5.159329350079567e-05
    de time 3.23 distance 0.00025199256577354556
    de time 4.31 distance 9.995401175424967e-05
    de time 5.38 distance 5.363785732115378e-05
    de time 6.45 distance 0.00025724959626183264
    de time 7.53 distance 5.30887897939869e-05
    de time 8.62 distance 0.00014133570932223227
    de time 9.71 distance 3.105216642390607e-05
    de time 10.81 distance 0.00020801779378730456
    
    de mean distance = 0.00012240097261537604
    de std distance = 8.258724841147236e-05
----

Note that we didn't use the alternative C++ implementation of DE fcmaes offers, because
parallel function evaluation is slower in this specific application context.  

==== CMA-ES using Parallel Fitness Evaluation 

The fcmaes CMA-ES implementation also offers parallel fitness evaluation, which we use here:

[source,python]
----  
    def opt_differential_evolution_loop(fits):
        ...
        for fit in fits: 
            ret = cmaes.minimize(fit, fit.bounds, input_sigma=0.7, 
                        max_evaluations = 1000, stop_fitness = 0.00001, workers=16)            ...
----

The results are similar to the one for fcmaes-DE, but slightly worse:

[source,python]
----   
    cmaes time 0.51 distance 0.026598245672092756
    cmaes time 1.55 distance 0.004407006710436312
    cmaes time 2.59 distance 7.25900467107854e-05
    cmaes time 3.65 distance 0.0001793522710383244
    cmaes time 4.69 distance 0.00016477295389366597
    cmaes time 5.78 distance 9.85274772162259e-05
    cmaes time 6.83 distance 0.00011994884350791102
    cmaes time 7.87 distance 5.438697928394909e-05
    cmaes time 8.92 distance 4.5451310954014446e-05
    cmaes time 9.98 distance 0.00014988796844872532
    
    cmaes mean distance = 0.003189017023358267
    cmaes std distance = 0.007907630855455892
----


==== BiteOpt using Parallel Optimization Retry 

https://github.com/avaneev/biteopt[BiteOpt] is written in C++ and doesn't support parallel
fitness evaluation as the two algorithms before. It is a very good choice if applied single threaded
or in the context of multiple parallel optimization retries. This approach sacrifices performance
for reliability. 

[source,python]
----  
def opt_biteopt_loop(fits):
    ...
    for fit in fits:  
        ret = retry.minimize(fit, fit.bounds, logger = None, 
                              num_retries=16, optimizer=Bite_cpp(100), workers=16)
    ...
----

The results are worse than the ones for differential evolution above, but BiteOpt may turn out
superior for harder optimization problems / larger quantum algorithms. It also is an excellent choice when applied
single threaded for multi-threaded/GPU simulations, then multiple sequential retries may be required
to obtain a reliable result. 

[source,python]
----   
    bite time 1.96 distance 7.608794221475312e-05
    bite time 3.85 distance 0.0001484067064991823
    bite time 5.76 distance 0.00145199256577358
    bite time 7.65 distance 9.995401175424967e-05
    bite time 9.57 distance 0.0001463621426788242
    bite time 11.48 distance 5.724959626185466e-05
    bite time 13.38 distance 0.0002530887897940204
    bite time 15.23 distance 0.00014133570932223227
    bite time 17.18 distance 3.105216642390607e-05
    bite time 19.11 distance 0.0009919822062127437
    
    bite mean distance = 0.0003397511836935346
    bite std distance = 0.000456672806132943
----

==== Search for Weaknesses of Optimization Algorithms

Finally we apply parallel optimization to search for weaknesses of COBYLA.
This means the fitness function performs a COBYLA optimization and searches
for a target distribution and an initial guess maximizing the final distance 
COBYLA returns:  

[source,python]
----  
    def find_COBYLA_weakness():
        
        def fitness(x):
            params = x[:3] # use first three decision variables as guess for COBYLA
            target_distr =  x[3:] # use two decision variables as target
            ...     
            fit = Fitness(target_distr)
            ret = COBYLA(maxiter=50000, tol=0.00001).minimize(fun=fit, x0=params)
            return -ret.fun # we maximize the distance
            
        bounds = Bounds([0]*5, [2]*3 + [1]*2)
        ret = de.minimize(wrapper(fitness), 5, bounds, max_evaluations = 300, workers=16)
        print("worst COBYLA distance = " +  str(ret.fun))
----

We get

[source,python]
----
0.61 16 26.0 -0.16048111250104818 [1.5150607763293302, 0.855535102710067, 0.9443911196710082, 0.44315944374947586, 0.5568405562505241]
0.71 19 27.0 -0.3451956010096087 [1.2667012355920217, 0.6074735090610092, 0.0, 0.5760978005048043, 0.4239021994951956]
0.72 20 28.0 -0.39053196482825997 [1.819039398258975, 0.5500857586958143, 0.473575221093664, 0.39823401758587, 0.6017659824141299]
0.74 21 28.0 -0.4871209992729757 [1.817252059960992, 0.0, 0.43090691832695105, 0.6197604996364878, 0.3802395003635122]
1.38 33 24.0 -1.5241888666705452 [2.0, 0.9968046393254453, 0.6468567010495877, 0.7675944333352724, 0.2324055666647275]
2.94 70 24.0 -1.7722 [2.0, 1.164716055769154, 0.8828197082450224, 1.0, 0.0]
5.14 117 23.0 -1.7860626205507981 [2.0, 1.260050169662025, 1.1363082378155278, 0.996531310275399, 0.003468689724600905]
7.27 163 22.0 -1.8356 [2.0, 1.5650815562087286, 0.6690050055417369, 1.0, 0.0]
8.69 197 23.0 -1.9714 [1.9104317884199735, 1.7238262098841646, 0.6395695514606753, 1.0, 0.0]
worst COBYLA distance = 1.9714
----

confirming that we can get bad optimization results using COBYLA.

=== Summary

- The COBYLA algorithm proposed in https://qiskit.org/textbook/ch-applications/vqe-molecules.html#Example-with-a-Single-Qubit-Variational-Form[Example-with-a-Single-Qubit-Variational-Form] is quite unreliable even for a 3-dimensional single qubit related optimization problem. 
- Alternatives adding reliability may converge slower.
- To mitigate this parallel optimization or parallel fitness evaluation can be used. 
- All three fcmaes-algorithms tested work reliably for the single qubit variational form optimization problem.
- Differential Evolution with parallel fitness evaluation offers the best time / reliability compromise.
- For bigger quantum algorithms "qasm_simulator", "aer_simulator_statevector" and "aer_simulator"
  work well with multithreading/GPU switched off, so we can profit from the scaling parallel optimization provides. 
- BiteOpt with parallel optimization retry is a very reliable option for more complex quantum related optimizations. 
- Differential Evolution with parallel function evaluation can be utilized as a meta-optimizer searching for weaknesses of optimization algorithms.

== VQE (Variational Quantum Eigensolver)

Lets see if what we learned so far is applicable to a bigger example, the parameterization of a VQE to solve
the maxcut graph problem. Whether to solve maxcut this way is a good idea is questionable for
two reasons:

- Classical solvers can compete quite well because of recent improvement in that area. 
- Noise produced by quantum gates are currently a real issue.  

But nevertheless this problem can serve as a benchmark for the optimization of VQEs which is useful in many other areas.
When using https://qiskit.org/documentation/optimization/tutorials/06_examples_max_cut_and_tsp.html[06_examples_max_cut_and_tsp.html] as a basis we noticed that there are several 
major qiskit refactorings ongoing and not all tutorials are adapted yet, so we had to change several imports.

=== Compare Different Optimizers

Suppose our task it to compare different optimizers for the maxcut VQE. Preliminary tests revealed that the 
fcmaes optimizers used in the first section don't work well here. So we are finally faced with two options:

- SPSA from qiskit.
- CR-FM-NES (Fast Moving Natural Evolution Strategy for High-Dimensional Problems, see https://arxiv.org/abs/2201.11422 ) from fcmaes.

How do we proceed? First we need to wrap CR-FM-NES as a qiskit optimizer so that qiskits VQE implementation can use it:

This is done in `class fcmaes_Optimizer(optimizers.Optimizer)` in https://github.com/dietmarwo/fast-cma-es/blob/master/examples/maxcut.py[maxcut.py]. Our wrapper is generic in two ways:

- It can consume any fcmaes optimizer
- It has a parameter `max_retries`. If `max_retries > 1` the fcmaes parallel optimization retry mechanism is applied.    
  Additionally we can restrict the number of parallel workers using parameter `workers`.

Lets also take the opposite route, wrapping SPSA as fcmaes algorithm as follows:

[source,python]
----
    class fcmaes_SPSA(Optimizer):
    
        def __init__(self, maxiter=1000):        
            Optimizer.__init__(self, maxiter, 'SPSA')
            self.opt = SPSA(maxiter=maxiter) # guessing
    
        def minimize(self, fun, bounds, guess=None, sdevs=None, rg=None, store=None):
            if guess is None: # necessary for parallel retry
                guess = np.random.uniform(bounds.lb, bounds.ub) if rg is None else \
                        rg.uniform(bounds.lb, bounds.ub)    
            ret = self.opt.minimize(fun, guess, bounds=[t for t in zip(bounds.lb, bounds.ub)])
            return ret.x, ret.fun, ret.nfev
----  

Now we can also execute SPSA in parallel by using a "double-wrapping" configuring 8000 evaluations / 4000 iterations:

[source,python]
----
    optimizer = fcmaes_Optimizer(fcmaes_SPSA(4000), max_retries = 16)
----

Note that fcmaes parallel retry doesn't forward an initial guess `x0` to improve the diversity of the runs. 
We take care of that by generating a random guess in this case. Other parameters of `minimize` are:

- `sdevs` the initial step size used to narrow / widen the search performed by the optimizer. CMA-ES and CR-FM-NES use this parameter. Lowering this value helps to enhance diversity of parallel retries. 
- `rg` a random generator, helps to preserve diversity if multiple processes are involved. 
- `store` used to maintain optimization results for statistical evaluation. 

=== Experiments

Execution of the experiments can be reproduced by adapting/executing https://github.com/dietmarwo/fast-cma-es/blob/master/examples/maxcut.py[maxcut.py]. You need fcmaes version 1.4.3 for these experiments (do `pip install fcmaes --upgrade`).

Similar to what is done in the original qiskit tutorial
https://qiskit.org/documentation/optimization/tutorials/06_examples_max_cut_and_tsp.html[06_examples_max_cut_and_tsp.html].
we solve the maxcut graph problem, but instead of a tiny graph we increase the number of graph nodes to `n=14`.

[source,python]
----
    G = nx.dense_gnm_random_graph(n, 2*n, seed=123)
    for (u, v) in G.edges():
        G.edges[u,v]['weight'] = 1
----

is used to generate an random 14 node graph using a fixed seed. Using a bigger graph will result in different timings, as the processor cache size becomes the dominating resource limitation. This graph results in 84 continuous decision variables. 

image::graph14.png[]

All our experiments were executed on a 16 core AMD 5950x CPU with 128 GB RAM on Linux Mint 20.3.  

Lets compare SPSA with CR-FM-NES trying to minimize execution time. This means we apply parallel
fitness evaluation to CR-FM-NES and use SPSA unmodified since parallel fitness evaluation requires support from the base algorithm. 

==== maxcut(SPSA(maxiter=4000), n, "aer_simulator")

As a reference lets first try "aer_simulator" without noise using SPSA.
We get 

[source,python]
----
energy: -7.9980468749999964
time: 313.02003717422485
max-cut objective: -21.998046874999996
----

which means the optimization nearly found the optimum in about 313 seconds. Next a single run using CR-FM-NES utilizing
parallel fitness evaluation.

==== maxcut(fcmaes_Optimizer(Crfmnes_cpp(8000, popsize=16, workers=16), use_wrapper=True), n, "aer_simulator")

`Crfmnes_cpp` uses the evaluation- not the iteration-number as parameter, so we have to 
use `8000` instead of `4000` to configure the same number of function evaluations. 

[source,python]
----
energy: -8.000000000000002
time: 26.00557279586792
max-cut objective: -22.0
----

This means there is factor >  10 speed gain by parallel fitness evaluation. As we can derive from the results below, the algorithm overhead from CR-FM-NES and SPSA itself is quite similar, but SPSA doesn't support parallel fitness evaluation. 

Using parameter `use_wrapper=True` we configured monitoring of the optimization resulting in output like:

[source,python]
----
24.68 7152 290.0 -8.000000000000002 [1.3432903811499826, -2.9505684864231103, ...]
----

Where we have:

- 24.68 : execution time in seconds.
- 7152 : number of function evaluations so far.
- 290.0 : number of function evaluations per second (eval/sec).
- -8.000000000000002 : best function value so far.
- [1.3432903811499826, -2.9505684864231103, ...] : best solution vector so far.

Adding noise we get:

==== maxcut(SPSA(maxiter=4000), n, "aer_simulator", add_noise=True)

[source,python]
----
energy: -7.749023437499998
time: 522.2729697227478
max-cut objective: -21.7490234375
----

==== maxcut(fcmaes_Optimizer(Crfmnes_cpp(8000, popsize=16, workers=16), use_wrapper=True), n, "aer_simulator", add_noise=True)

[source,python]
----
energy: -7.7685546875
time: 241.8653633594513
max-cut objective: -21.7685546875
----

This means now there is a factor 2.16 speed gain by parallel fitness evaluation. Much less than without noise where it was
factor > 10.   

We found good optimization results, but how do we check how reliable these are? We need a multi-restart/retry of the optimization. 
Fortunately this can be parallelized using fcmaes also for SPSA. But why do a parallel retry also for CR-FM-NES? 
We have parallel function evaluation and could apply this mode in a loop. Reason is that parallelization introduces
an overhead which grows when the tasks are shorter. An optimization lasts much longer than a fitness evaluation, so the
overhead shrinks which means we get better scaling: The execution of all optimizations needs less time,

- Use parallel fitness evaluation mode if you want a fast result.
- Use parallel optimization retry if you want to test the reliability of an algorithm. 

====  maxcut(fcmaes_Optimizer(fcmaes_SPSA(4000), max_retries = 16, use_wrapper=True, logger=logger()), n, "aer_simulator")

[source,python]
----
441.87 289 16 128000 -8.000000 -7.66 0.58 [-8.0, -8.0, -7.998, -7.9961, -7.9932, -7.9912, -7.9912, -7.9873, -7.9873, -7.9805, -7.9727, -7.6709, -6.9912, -6.9893, -6.9814, -5.9922] [4.16771830053249, 6.088996100937207, ...]
energy: -8.000000000000002
time: 441.8702871799469
max-cut objective: -22.0
----

Parameter `logger=logger()` is used to monitor the results we get from the different parallel runs:

- 441.87 : execution time in seconds.
- 289 : number of function evaluations per second (eval/sec).
- 16 : number of finished optimization runs.
- 128000 : number of function calls.
- -8.000000 : best function value so far.
- -7.66 : mean value of all optimization results.
- -0.58 : standard deviation of all optimization results.
- - [-8.0, -8.0, -7.998, -7.9961, -7.9932,, ...] list of best optimization results.

So we have for SPSA(4000):

- 442 seconds for 16 runs resulting in energy =  `[8.0, 8.0, 7.998, 7.9961, 7.9932, 7.9912, 7.9912, 7.9873, 7.9873, 7.9805, 7.9727, 7.6709, 6.9912, 6.9893, 6.9814, 5.9922]`

For CR-FM-NES we get:
 
==== maxcut(fcmaes_Optimizer(Crfmnes_cpp(8000, popsize=16), max_retries = 16, use_wrapper=True, logger=logger()), n, "aer_simulator")

[source,python]
----
333.24 384 16 128000 -8.000000 -7.50 0.71 [-8.0, -8.0, -8.0, -7.998, -7.998, -7.998, -7.998, -7.9941, -7.9932, -7.9912, -7.001, -7.0, -7.0, -7.0, -5.998, -5.998] [1.8212847242570533, -2.2382899214165737, ...]
energy: -8.000000000000004
time: 333.24353432655334
max-cut objective: -22.000000000000004
----

- 333 seconds for 16 runs resulting in energy = `[8.0, 8.0, 8.0, 7.998, 7.998, 7.998, 7.998, 7.9941, 7.9932, 7.9912, 7.001, 7.0, 7.0, 7.0, 5.998, 5.998]`

Comparing the 16 results for both optimizers we notice:

- SPSA is slightly slower when parallelized compared to CR-FM-NES.
- SPSA still saves a lot of time when parallelized (442 sec compared to 16*313 sec = factor 11.3)
- Optimization results for the 16 runs are roughly comparable between CR-FM-NES and SPSA (8000 fitness evaluations each). 

Adding noise we perform only 8 runs since execution is much slower and get for SPSA:

====  maxcut(fcmaes_Optimizer(fcmaes_SPSA(4000), max_retries = 8, use_wrapper=True, logger=logger()), n, "aer_simulator", add_noise=True)


[source,python]
----
1792.22 35 8 64000 -7.755859 -7.60 0.32 [-7.7559, -7.7549, -7.7363, -7.7256, -7.7188, -7.7129, -7.6562, -6.7461] [1.321933480266777, -6.299646940427553, ...]
----

- 1792 seconds for 8 runs resulting in energy = `[7.7559, 7.7549, 7.7363, 7.7256, 7.7188, 7.7129, 7.6562, 6.7461]`
For CR-FM-NES with noise we get using 8 parallel retries:

==== maxcut(fcmaes_Optimizer(Crfmnes_cpp(8000, popsize=16), max_retries = 8, use_wrapper=True, logger=logger()), n, "aer_simulator", add_noise=True)

[source,python]
----
1798.09 35 8 64000 -7.790039 -7.10 0.89 [-7.79, -7.7725, -7.7422, -7.7217, -7.6836, -6.7832, -5.7119, -5.585] ...
energy: -7.7900390625
time: 1798.0911939144135
max-cut objective: -21.7900390625
----

- 1798 seconds for 8 runs resulting in energy = `[7.79, 7.7725, 7.7422, 7.7217, 7.6836, 6.7832, 5.7119, 5.585]`

Comparing the 8 results for both optimizers we notice:

- SPSA and CR-FM-NES need the same time, it seems generating noise is the dominating factor. 
- Parallel optimization saves a lot of time compared to a serial restart: 1792 sec compared to 16*522 sec: Factor 4.66. 
This confirms that parallel optimization scales better than parallel function evaluation (factor 2.16). 
But the advantage is smaller than without noise, since noise generation is itself multi-threaded. 
- Optimization results for the 8 runs differ slightly, CR-FM-NES has higher standard deviation and best energy found, SPSA has a
better mean value. Even the second best CR-FM-NES energy value is better than the best SPSA one. 

The mean result can be improved by increasing the population size parameter without a significant slow down: 

[source,python]
----
==== maxcut(fcmaes_Optimizer(Crfmnes_cpp(8000, popsize=24), max_retries = 8, use_wrapper=True, logger=logger()), n, "aer_simulator", add_noise=True)

1831.95 35 8 64128 -7.805664 -7.36 0.49 [-7.8057, -7.7822, -7.7588, -7.666, -7.6455, -6.7695, -6.7354, -6.6982] ... 
energy: -7.805664062499999
time: 1831.9511473178864
max-cut objective: -21.8056640625
----

- 1832 seconds for 8 runs resulting in energy = `[7.8057, 7.7822, 7.7588, 7.666, 7.6455, 6.7695, 6.7354, 6.6982]`

Finally lets try another simulator: "aer_simulator_statevector":

We reduce the size of the graph to 13 nodes and use only 2000 evaluations.

==== maxcut(SPSA(maxiter=1000), n, "aer_simulator_statevector")

[source,python]
----
solution objective: 20.0
energy: -6.993037078357363
time: 306.25169563293457
---- 

Execution is very slow. To get this result we needed to use:

[source,python]
----
    with threadpoolctl.threadpool_limits(limits=1, user_api="blas"): 
        # blas threading restriction, speeds up "aer_simulator_statevector" 
        result = vqe.compute_minimum_eigenvalue(qubitOp)
---- 
  
We found no other way to restrict usage of multiple threads by configuration:

[source,python]
----
    backend.set_options(max_parallel_threads=1)
----

just did't work. But why is this a good idea in case we only want to execute a single optimization?

==== maxcut(SPSA(maxiter=1000), n, "aer_simulator_statevector") without threadpool_limits(limits=1)

[source,python]
----
solution objective: 20.0
energy: -6.989098658672954
time: 338.2265610694885
----

It is because all these threads slow down the computation. It is not clear why, may be this is cache size related.
Nevertheless there is a "positive" side effect: CPU temperature is 20 degree higher - so we may call
this configuration "room heater mode". During summer this may not be what you want to do, as long 
as there is no speed-up. 

"aer_simulator_statevector" is not well suited for parallelization, scaling is even worse than with "aer_simulator" + noise. 

==== Exercises

- Test "qasm_simulator" with and without noise.
- Can you find a single threaded noise generator? Expectation is that both parallelization modes work much better 
if the whole quantum simulation including noise generation is single threaded. 

=== Summary

- SPSA is an excellent algorithm for VQE optimization, specially if noise is involved. 
- CR-FM-NES (Fast Moving Natural Evolution Strategy for High-Dimensional Problems, see 
https://arxiv.org/abs/2201.11422) is a good alternative which may have advantages for more complex VQEs. 
- Both can be parallelized using fcmaes parallel restart/retry resulting in very good scaling, specially if there is no noise. 
- Noise greatly reduces what can be gained by parallelization, probably because noise generation is multi-threaded.
- Using parallel function evaluation CR-FM-NES is much faster than SPSA, specially if there is no noise. 
- Maxcut parallelization scaling is best for "qasm_simulator" and "aer_simulator" without noise with less or equal 14 graph nodes.
- For bigger graphs either cache size becomes the dominating performance limit or the simulation performs some internal 
  multi-threading only for very large circuits.   
- Avoid the "room heater", don't allow "aer_simulator_statevector" used with VQEs to use > 1 thread, even without parallel optimization. At least if it is not winter time.

  