:encoding: utf-8
:imagesdir: img
:cpp: C++

== Optimize with Expressions

To cite https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/[NLopt_Algorithms]:
"For any given optimization problem, it is a good idea to compare several of the available algorithms that are applicable to that problemâ€”in general, one often finds that the "best" algorithm strongly depends upon the problem at hand."

This is true, but there is more to this issue. In the literature we find:

* https://www.sciencedirect.com/science/article/abs/pii/S2210650218301585[Speed up differential evolution]:
"To do so, CMAES is applied to the best known solution provided by DE. If the solution found by CMAES is better than the solution obtained in the exploration phase by DE, then it replaces the old one"

* https://www.springerprofessional.de/design-of-robust-space-trajectories/1770072[Design of Robust Space Trajectories]: 
"... a search procedure with an algorithm
selected among DE, CMA-ES and DIRECT is performed..."

Not always a single optimization algorithm is optimal for a given problem, sometimes it is better to combine different algorithms. Two basic combination-concepts can be derived from the approaches above:

* A sequence of optimization algorithms
* A random choice between optimization algorithms

To generalize even further: Why not allowing arbitrary expressions with random choice and sequence operators? fcmaes supports this idea:

[source,python]
----
from fcmaes.optimizer import Choice, Sequence, Cma_cpp, De_cpp
from fcmaes import retry
...
alg1 = Cma_cpp(50000)
alg2 = De_cpp(50000)
alg3 = Sequence([alg2, alg1])
alg4 = Choice([alg3, alg1])

ret = retry.minimize(fun, bounds, num_retries = 2000, logger = logger(), optimizer = alg4)
----

These expressions can be forwarded to the parallel or the coordinated parallel retry of fcmaes. 
Each base algorithm has its own individually assigned evaluation budget - 50000 evaluations in the example above. With the coordinated parallel retry (`from fcmaes import advretry`) these budgets
are individually incremented, their relation stays the same. 

=== Optimization algorithms for fcmaes parallel retry

There are five types of `basic` algorithms which form the basis of expressions:

* fcmaes native algorithms: `Cma_cpp, De_cpp, Da_cpp, Cma_python`, CMA-ES, differential evolution and dual annealing
* fcmaes predefined sequences: `de_cma, da_cma`, where `de_cma`, the sequence `DE -> CMA-ES` is the default algorithm used for the parallel retry.  
* https://docs.scipy.org/doc/scipy/reference/optimize.html[scipy algorithms]: `Dual_annealing, Differential_evolution, Basin_hopping, Minimize, Shgo`
* https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/[NLopt algorithms]: `NLopt`, requires `pip install nlopt`

Example: 

[source,python]
----
import nlopt
from fcmaes.optimizer import NLopt
dim = 18
dir = nlopt.opt(nlopt.GN_DIRECT_L, dim)
esch = nlopt.opt(nlopt.GN_ESCH, dim)
bob = nlopt.opt(nlopt.LN_BOBYQA, dim)
sbp = nlopt.opt(nlopt.LN_SBPLX, dim)
algo = Choice([NLopt(dir, 2000), NLopt(esch, 2000), NLopt(bob, 2000), NLopt(sbp, 2000)])
----

Note that using `NLopt` currently is the only way to use algorithms supporting equality and inequality constraints in combination with the coordinated retry `fcmaes.advretry`. 

* https://esa.github.io/pagmo2/docs/cpp/cpp_docs.html#implemented-algorithms[Pagmo] single objective algorithms: requires `pip install pygmo`

There is no predefined general algorithm wrapper, but you may define individual wrappers:

Example: 

[source,python]
----
import pygmo as pg
from fcmaes.optimizer import Optimizer, de_cma

class SADE_pagmo(Optimizer):
    """SADE pagmo."""
   
    def __init__(self, max_evaluations=50000,
                 popsize = 20, guess=None, stop_fittness = None):        
        Optimizer.__init__(self, max_evaluations, 'SADE pagmo')
        self.popsize = popsize

    def minimize(self, fun, bounds, guess=None, sdevs=0.3, rg=Generator(MT19937()), store=None):       
        problem = pg.problem(pagmo_problem(fun, bounds, "pagmo function"))
        maxevals = self.max_eval_num(store)
        pgalgo = pg.algorithm(
            pg.sade(maxevals//self.popsize, seed = int(rg.uniform(0, 2**32 - 1))))
        pop = pg.population(problem, self.popsize)
        pop = algo.evolve(pop) 
        return pop.champion_x, pop.champion_f, pop.problem.get_fevals()

algo = Choice([SADE_pagmo(50000), de_cma(50000)])
----

Note that although https://esa.github.io/pagmo2/[pagmo] algorithm wrappers can only be defined for unconstrained single objective algorithms, https://github.com/dietmarwo/fast-cma-es/blob/master/Constraints.adoc[Constraints] shows two ways to parallelize all `pagmo` algorithms:

- using `pg.archipelago`, pagmos own parallelization method 
- using `fcmaes.pygmoretry`, a parallel retry mechanism specific for `pagmo` algorithms. 

But for the coordinated retry a wrapper is required. Use `NLopt` instead if you want to apply the coordinated retry with algorithms supporting equality and inequality constraints.
