:encoding: utf-8
:imagesdir: img

= fcmaes - a Python 3 gradient-free optimization library

fcmaes complements https://docs.scipy.org/doc/scipy/reference/optimize.html[scipy optimize] by providing 
additional optimization methods, faster C++/Eigen based implementations and a coordinated parallel retry mechanism. 
It supports the multi threaded application of different gradient free optimization algorithms. 

=== Documentation

- https://github.com/dietmarwo/fast-cma-es/blob/master/Tutorial.adoc[Tutorial]
- https://github.com/dietmarwo/fast-cma-es/blob/master/Results.adoc[Results]

=== Features

- fcmaes is focussed on optimization problems hard to solve.
- minimized algorithm overhead - relative to the objective function evaluation time - even for high dimensions. 
- parallel coordinated retry of sequences of optimization algorithms
- parallel function evaluation for CMA-ES
- new differential evolution variant optimized for usage with parallel coordinated retry and the DE->CMA sequence. 
- on modern many-core machines global minima can usually be found fast using the default parameters.  

Since fcmaes only supports boxed constraints other constraints have to be mapped to penalty functions. This is much
easier with gradient free optimization algorithms.

Although binaries for Windows are provided, we highly recommend using Linux since fcmaes Python multithreading is less performant on Windows. But the Linux subsystem for windows works fine if you want to execute optimizations on a Windows machine. 

=== Optimization algorithms

- CMA-ES: Implemented both in Python and in C++. The Python version is faster than
https://github.com/CMA-ES/pycma/tree/master/cma[CMA] but slower than the C++ variant. Only the Python variant provides an ask/tell interface and supports parallel function evaluation. Both CMA variants provide less configurability than https://github.com/CMA-ES/pycma/tree/master/cma[CMA].

- Dual Annealing: Implemented in C++. Use the scipy implementation if you prefer a pure python variant or need more
configuration options. 

- Differential Evolution: Implemented in C++. There is no python equivalent, the scipy implementation 
works differently. Additional concepts implemented here are 
https://www.researchgate.net/publication/309179699_Differential_evolution_for_protein_folding_optimization_based_on_a_three-dimensional_AB_off-lattice_model[temporal locality] and stochastic reinitialization of individuals based on their age. 

- Sequences: It is possible to define sequences of optimization algorithms. Not only the three algorithms above, but also
many scipy optimization methods and custom algorithms can be used in sequences. Default method for the parallel retry is the sequence DE -> CMA with 80% of the evaluation budget assigned to CMA.

=== Dependencies

Runtime:

- numpy: https://github.com/numpy/numpy
- scipy: https://github.com/scipy/scipy

Compile time (binaries for Linux and Windows are included):

- Eigen https://gitlab.com/libeigen/eigen (version >= 3.9 is required for CMA).
- pcg-cpp: https://github.com/imneme/pcg-cpp - used in all c++ optimization algorithms.
- LBFGSpp: https://github.com/yixuan/LBFGSpp/tree/master/include - used for dual annealing local optimization.

 
=== Installation

==== Linux
 
* `pip install fcmaes`

==== Windows

* `pip install fcmaes`
* install C++ runtime libraries https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads

===== Recommended alternative:
* Linux subsystem for Windows, see
https://docs.microsoft.com/en-us/windows/wsl/install-win10[Linux subsystem] or
https://superuser.com/questions/1271682/is-there-a-way-of-installing-ubuntu-windows-subsystem-for-linux-on-win10-v170[Ubuntu subsystem].
Tested with Ubuntu 18.04, fcmaes performance is about 98% of a native Linux installation.

==== MacOS

* `pip install fcmaes`
* For using the C++ optimization algorithms: 
** adapt https://github.com/dietmarwo/fast-cma-es/blob/master/_fcmaescpp/CMakeLists.txt[CMakeLists.txt]
** compile https://github.com/dietmarwo/fast-cma-es/tree/master/_fcmaescpp[_fcmaescpp] 

=== Usage

Usage is similar to https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html[scipy.optimize.minimize].

[source,python]
----
from fcmaes import cmaes
ret = cmaes.minimize(fun, bounds, x0)
print (ret.x, ret.fun, ret.nfev)
----

fcmaes.cmaes.minimize cannot handle constraints and derivatives (jac, hess, hessp). If the initial guess x0 is undefined,
a feasible uniformly distributed random value is automatically generated. It is recommended to define
bounds, since CMA-ES uses them also for internal scaling. Additional parameters are

- `popsize` (default 31) - Size of the population used. Instead of increasing this parameter for hard problems, it is often better to use parallel retry instead. Reduce `popsize` for a narrower search if your budget is restricted.
- `input_sigma` (default 0.3) - The initial step size. Can be defined for each dimension separately. Both parallel retry mechanism
  set this parameter together with the initial guess automatically.   
- `is_parallel` (default False) - Calls the objective function for the whole population in parallel. Useful for costly 
  objective functions but is switched off when you use parallel retry.    
  
For the C++ variant use instead:

[source,python]
----
from fcmaes import cmaescpp
ret = cmaescpp.minimize(fun, bounds, x0)
----

Alternatively there is an ask/tell interface to interact with CMA-ES:

[source,python]
----
es = cmaes.Cmaes(bounds, x0)
for i in range(iterNum):
    xs = es.ask()
    ys = [fun(x) for x in xs]
    status = es.tell(ys)
    if status != 0:
        break 
----

Differential evolution (fcmaes.decpp) and Dual Annealing (fcmaes.dacpp) provide similar interfaces. 
[source,python]
----
from fcmaes import decpp, dacpp
ret = decpp.minimize(fun, bounds)
ret = dacpp.minimize(fun, bounds, x0)
----

For simple parallel retry use:

[source,python]
----
from fcmaes.optimizer import logger
from fcmaes import retry
ret = retry.minimize(fun, bounds, logger=logger())
----

For coordinated parallel retry use:

[source,python]
----
from fcmaes.optimizer import logger
from fcmaes import advretry
ret = advretry.minimize(fun, bounds, logger=logger())
----

Parallel retry does not support initial quess `x0` and initial step size `input_sigma` parameters because it
uses generated guesses and step size values. Use parameter `logger` to specify the 
log output, default is no logging. Use 
`fcmaes.optimizer import logger` to log both into a file and to stdout. 
Check the https://github.com/dietmarwo/fast-cma-es/blob/master/Tutorial.adoc[Tutorial] for more details. 
It is possible to use other optimization methods with parallel retry, see
https://github.com/dietmarwo/fast-cma-es/blob/master/fcmaes/examples.py[examples.py],
https://github.com/dietmarwo/fast-cma-es/blob/master/fcmaes/advexamples.py[advexamples.py] and
https://github.com/dietmarwo/fast-cma-es/blob/master/fcmaes/optimizer.py[optimizer.py] 

=== Performance

On a single AMD 3950x CPU using https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh[Anaconda 2019.10]
for Linux the parallel coordinated retry mechanism 
solves ESAs 26-dimensional https://www.esa.int/gsp/ACT/projects/gtop/messenger_full/[Messenger full] problem
in about 2 hours on average. The Messenger full benchmark models a
multi-gravity assist interplanetary space mission from Earth to Mercury. In 2009 the first good solution (6.9 km/s)
was submitted. It took more than five years to reach 1.959 km/s and three more years until 2017 to find the optimum 
http://www.midaco-solver.com/index.php/component/content/article?id=208[1.958 km/s]. The picture below shows the
progress of the whole science community since 2009:

image::Fsc.png[]  

The following picture shows 173 retry runs, each about 1 hour. 

image::mf3.6000.png[]  

91 out of these 173 runs produced a result better than 2 km/s:

image::mf3.2000.png[] 

69, more than a third reached the absolute minimum at 1.958 km/s. 

image::mf3.1959.png[] 

Using fcmaes with parallel retry performs > 800000 messenger_full evaluations per second
on an AMD 3950x processor. This outperforms both the official
https://github.com/CMA-ES/pycma[CMA-ES] implementation and scipy differential evolution.

=== How to read the log output of the parallel retry
The log output of the parallel retry contains the following rows:

===== Simple retry

- time (in sec)
- evaluations / sec
- number of retries - optimization runs
- total number of evaluations in all retries
- best value found so far
- mean of the values found by the retries below the defined threshold
- standard deviation of the values found by the retries below the defined threshold
- list of the best 20 function values in the retry store
- best solution (x-vector) found so far

Mean and standard deviation would be misleading when using advanced retry, because
of the retries initiated by crossover. Therefore the rows of the
log output differ slightly:
 
===== Advanced coordinated retry

- time (in sec)
- evaluations / sec
- number of retries - optimization runs
- total number of evaluations in all retries
- best value found so far
- worst value in the retry store
- number of entries in the retry store
- list of the best 20 function values in the retry store
- best solution (x-vector) found so far
